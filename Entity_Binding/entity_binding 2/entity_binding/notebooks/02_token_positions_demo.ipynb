{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Position Functions for Entity Binding\n",
    "\n",
    "This notebook demonstrates the **template-aware token position system** for entity binding tasks. Token positions are crucial for causal interventions - they tell us **WHERE** in the transformer to intervene to test causal hypotheses.\n",
    "\n",
    "## Why Template-Aware Token Positions Matter\n",
    "\n",
    "The new system solves a critical problem: **ambiguity when entities appear multiple times**.\n",
    "\n",
    "Consider this prompt: `\"Pete loves jam, and Ann loves pie. What does Pete love?\"`\n",
    "\n",
    "- \"Pete\" appears **twice**: once in the statement, once in the question\n",
    "- The old system always found the **first occurrence**\n",
    "- The new system **understands the template structure** and can distinguish:\n",
    "  - Pete in the **statement** (tokens 0-1)\n",
    "  - Pete in the **question** (token 12)\n",
    "\n",
    "This notebook shows how to use the new system effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from causalab.tasks.entity_binding.config import (\n",
    "    create_sample_love_config,\n",
    "    create_sample_action_config,\n",
    ")\n",
    "from causalab.tasks.entity_binding.causal_models import (\n",
    "    create_direct_causal_model,\n",
    "    sample_valid_entity_binding_input,\n",
    ")\n",
    "from causalab.tasks.entity_binding.token_positions import (\n",
    "    PromptParser,\n",
    "    PromptTokenizer,\n",
    "    get_entity_token_indices_structured,\n",
    ")\n",
    "from causalab.neural.pipeline import LMPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Ambiguity Problem (Why We Need Structure)\n",
    "\n",
    "Let's start by demonstrating the problem that the new system solves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load language model\n",
    "print(\"Loading GPT-2 model...\")\n",
    "pipeline = LMPipeline(\"gpt2\")\n",
    "config = create_sample_love_config()\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Pete loves jam, and Ann loves pie. What does Pete love?\n",
      "\n",
      "Note: 'Pete' appears TWICE in this prompt:\n",
      "  1. In the statement: 'Pete loves jam'\n",
      "  2. In the question: 'What does Pete love?'\n",
      "\n",
      "Which 'Pete' do we want to intervene on?\n"
     ]
    }
   ],
   "source": [
    "# Create example where \"Pete\" appears in both statement and question\n",
    "input_sample = {\n",
    "    \"entity_g0_e0\": \"Pete\",\n",
    "    \"entity_g0_e1\": \"jam\",\n",
    "    \"entity_g1_e0\": \"Ann\",\n",
    "    \"entity_g1_e1\": \"pie\",\n",
    "    \"query_group\": 0,\n",
    "    \"query_indices\": (0,),\n",
    "    \"answer_index\": 1,\n",
    "    \"active_groups\": 2,\n",
    "    \"entities_per_group\": 2,\n",
    "    \"raw_input\": \"Pete loves jam, and Ann loves pie. What does Pete love?\",\n",
    "}\n",
    "\n",
    "print(\"Prompt:\", input_sample[\"raw_input\"])\n",
    "print(\"\\nNote: 'Pete' appears TWICE in this prompt:\")\n",
    "print(\"  1. In the statement: 'Pete loves jam'\")\n",
    "print(\"  2. In the question: 'What does Pete love?'\")\n",
    "print(\"\\nWhich 'Pete' do we want to intervene on?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full tokenization:\n",
      "Total tokens: 15\n",
      "\n",
      "Token  0: 'P'                  <-- FIRST Pete (statement)\n",
      "Token  1: 'ete'                <-- FIRST Pete (statement)\n",
      "Token  2: ' loves'            \n",
      "Token  3: ' jam'              \n",
      "Token  4: ','                 \n",
      "Token  5: ' and'              \n",
      "Token  6: ' Ann'              \n",
      "Token  7: ' loves'            \n",
      "Token  8: ' pie'              \n",
      "Token  9: '.'                 \n",
      "Token 10: ' What'             \n",
      "Token 11: ' does'             \n",
      "Token 12: ' Pete'              <-- SECOND Pete (question)\n",
      "Token 13: ' love'             \n",
      "Token 14: '?'                 \n"
     ]
    }
   ],
   "source": [
    "# Show full tokenization\n",
    "tokens = pipeline.load(input_sample)[\"input_ids\"][0].tolist()\n",
    "print(\"Full tokenization:\")\n",
    "print(f\"Total tokens: {len(tokens)}\\n\")\n",
    "\n",
    "for i, token_id in enumerate(tokens):\n",
    "    token_str = pipeline.tokenizer.decode([token_id])\n",
    "    if i in [0, 1]:\n",
    "        marker = \" <-- FIRST Pete (statement)\"\n",
    "    elif i == 12:\n",
    "        marker = \" <-- SECOND Pete (question)\"\n",
    "    else:\n",
    "        marker = \"\"\n",
    "    print(f\"Token {i:2d}: {repr(token_str):20s}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Template-Aware Parsing\n",
    "\n",
    "The new system **parses the prompt structure** using template knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed prompt structure:\n",
      "  Total segments: 11\n",
      "  Statement region: chars (0, 34)\n",
      "  Question region: chars (35, 55)\n",
      "\n",
      "The parser understands the template structure!\n"
     ]
    }
   ],
   "source": [
    "# Parse the prompt into structured segments\n",
    "parser = PromptParser(config)\n",
    "parsed = parser.parse_prompt(input_sample)\n",
    "\n",
    "print(\"Parsed prompt structure:\")\n",
    "print(f\"  Total segments: {len(parsed.segments)}\")\n",
    "print(f\"  Statement region: chars {parsed.statement_region}\")\n",
    "print(f\"  Question region: chars {parsed.question_region}\")\n",
    "print(\"\\nThe parser understands the template structure!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt broken down into segments:\n",
      "\n",
      " 0. [STMT] entity          | 'Pete'\n",
      " 1. [STMT] template_text   | ' loves '\n",
      " 2. [STMT] entity          | 'jam'\n",
      " 3. [STMT] delimiter       | ', and '\n",
      " 4. [STMT] entity          | 'Ann'\n",
      " 5. [STMT] template_text   | ' loves '\n",
      " 6. [STMT] entity          | 'pie'\n",
      " 7. [STMT] delimiter       | '.'\n",
      " 8. [QUES] question_text   | 'What does '\n",
      " 9. [QUES] entity          | 'Pete'\n",
      "10. [QUES] question_text   | ' love?'\n"
     ]
    }
   ],
   "source": [
    "# Show the parsed segments\n",
    "print(\"\\nPrompt broken down into segments:\\n\")\n",
    "for i, seg in enumerate(parsed.segments):\n",
    "    # Determine which region this segment belongs to\n",
    "    in_statement = (\n",
    "        parsed.statement_region\n",
    "        and parsed.statement_region[0] <= seg.char_start < parsed.statement_region[1]\n",
    "    )\n",
    "    in_question = (\n",
    "        parsed.question_region\n",
    "        and parsed.question_region[0] <= seg.char_start < parsed.question_region[1]\n",
    "    )\n",
    "    region = \"STMT\" if in_statement else (\"QUES\" if in_question else \"???\")\n",
    "\n",
    "    print(f\"{i:2d}. [{region}] {seg.segment_type:15s} | '{seg.text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Segments\n",
    "\n",
    "Each segment has a **type**:\n",
    "- **entity**: An entity from the binding matrix\n",
    "- **template_text**: Text from the statement template\n",
    "- **delimiter**: Punctuation between statements (`,` `, and`, `.`)\n",
    "- **question_text**: Text from the question template\n",
    "\n",
    "Notice that **both Pete segments are tracked** - one in the statement, one in the question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, each segment has token positions:\n",
      "\n",
      "Entity g0_e0 ('Pete'): tokens [0, 1]\n",
      "Entity g0_e1 ('jam'): tokens [3]\n",
      "Entity g1_e0 ('Ann'): tokens [6]\n",
      "Entity g1_e1 ('pie'): tokens [8]\n",
      "Entity g0_e0 ('Pete'): tokens [12]\n"
     ]
    }
   ],
   "source": [
    "# Add token positions to the parsed structure\n",
    "tokenizer = PromptTokenizer(pipeline)\n",
    "parsed = tokenizer.tokenize_prompt(parsed)\n",
    "\n",
    "print(\"After tokenization, each segment has token positions:\\n\")\n",
    "for seg in parsed.segments:\n",
    "    if seg.segment_type == \"entity\":\n",
    "        print(\n",
    "            f\"Entity g{seg.group_idx}_e{seg.entity_idx} ('{seg.text}'): tokens {seg.token_positions}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Region-Aware Token Position Queries\n",
    "\n",
    "The key feature: **specify which region** you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pete in STATEMENT: tokens [0, 1]\n",
      "Pete in QUESTION:  tokens [12]\n",
      "Pete first occurrence (region=None): tokens [0, 1]\n",
      "\n",
      "======================================================================\n",
      "KEY INSIGHT: We can now distinguish between duplicate entities!\n",
      "======================================================================\n",
      "✓ Verified: Statement and question Petes have different token positions\n"
     ]
    }
   ],
   "source": [
    "# Get Pete from the STATEMENT region\n",
    "pete_statement = get_entity_token_indices_structured(\n",
    "    input_sample, pipeline, config, 0, 0, region=\"statement\"\n",
    ")\n",
    "print(f\"Pete in STATEMENT: tokens {pete_statement}\")\n",
    "\n",
    "# Get Pete from the QUESTION region\n",
    "pete_question = get_entity_token_indices_structured(\n",
    "    input_sample, pipeline, config, 0, 0, region=\"question\"\n",
    ")\n",
    "print(f\"Pete in QUESTION:  tokens {pete_question}\")\n",
    "\n",
    "# Get first occurrence (backward compatible)\n",
    "pete_first = get_entity_token_indices_structured(\n",
    "    input_sample, pipeline, config, 0, 0, region=None\n",
    ")\n",
    "print(f\"Pete first occurrence (region=None): tokens {pete_first}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY INSIGHT: We can now distinguish between duplicate entities!\")\n",
    "print(\"=\" * 70)\n",
    "assert pete_statement != pete_question, \"Statement and question should differ!\"\n",
    "print(\"✓ Verified: Statement and question Petes have different token positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visual Verification\n",
    "\n",
    "Let's visualize which tokens are selected for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-by-token breakdown:\n",
      "\n",
      "Token  0: 'P'                  <-- PETE(statement)\n",
      "Token  1: 'ete'                <-- PETE(statement)\n",
      "Token  2: ' loves'            \n",
      "Token  3: ' jam'              \n",
      "Token  4: ','                 \n",
      "Token  5: ' and'              \n",
      "Token  6: ' Ann'              \n",
      "Token  7: ' loves'            \n",
      "Token  8: ' pie'              \n",
      "Token  9: '.'                 \n",
      "Token 10: ' What'             \n",
      "Token 11: ' does'             \n",
      "Token 12: ' Pete'              <-- PETE(question)\n",
      "Token 13: ' love'             \n",
      "Token 14: '?'                 \n"
     ]
    }
   ],
   "source": [
    "# Show which tokens correspond to which occurrences\n",
    "pete_statement_tokens = get_entity_token_indices_structured(\n",
    "    input_sample, pipeline, config, 0, 0, region=\"statement\"\n",
    ")\n",
    "pete_question_tokens = get_entity_token_indices_structured(\n",
    "    input_sample, pipeline, config, 0, 0, region=\"question\"\n",
    ")\n",
    "\n",
    "print(\"Token-by-token breakdown:\\n\")\n",
    "for i, token_id in enumerate(tokens):\n",
    "    token_str = pipeline.tokenizer.decode([token_id])\n",
    "    markers = []\n",
    "    if i in pete_statement_tokens:\n",
    "        markers.append(\"PETE(statement)\")\n",
    "    if i in pete_question_tokens:\n",
    "        markers.append(\"PETE(question)\")\n",
    "    marker_str = \" <-- \" + \", \".join(markers) if markers else \"\"\n",
    "    print(f\"Token {i:2d}: {repr(token_str):20s}{marker_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: All Entities in the Prompt\n",
    "\n",
    "Let's find token positions for all entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_g0_e0': 'Pete', 'entity_g0_e1': 'jam', 'entity_g1_e0': 'Ann', 'entity_g1_e1': 'pie', 'query_group': 0, 'query_indices': (0,), 'answer_index': 1, 'active_groups': 2, 'entities_per_group': 2, 'raw_input': 'Pete loves jam, and Ann loves pie. What does Pete love?'}\n",
      "All entities in the prompt:\n",
      "\n",
      "Pete       in statement  -> tokens [0, 1]\n",
      "jam        in statement  -> tokens [3]\n",
      "Ann        in statement  -> tokens [6]\n",
      "pie        in statement  -> tokens [8]\n",
      "Pete       in question   -> tokens [12]\n"
     ]
    }
   ],
   "source": [
    "# Find all entities using the parsed structure\n",
    "print(input_sample)\n",
    "print(\"All entities in the prompt:\\n\")\n",
    "\n",
    "entities = [\n",
    "    (0, 0, \"Pete\", \"statement\"),\n",
    "    (0, 1, \"jam\", \"statement\"),\n",
    "    (1, 0, \"Ann\", \"statement\"),\n",
    "    (1, 1, \"pie\", \"statement\"),\n",
    "    (0, 0, \"Pete\", \"question\"),  # Pete appears again in question\n",
    "]\n",
    "\n",
    "for group_idx, entity_idx, name, region in entities:\n",
    "    try:\n",
    "        tokens = get_entity_token_indices_structured(\n",
    "            input_sample, pipeline, config, group_idx, entity_idx, region=region\n",
    "        )\n",
    "        print(f\"{name:10} in {region:10} -> tokens {tokens}\")\n",
    "    except ValueError:\n",
    "        print(f\"{name:10} in {region:10} -> NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Multi-Token Entities with Region Specification\n",
    "\n",
    "The system handles multi-token entities within specific regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-token example:\n",
      "Prompt: Elizabeth loves strawberry jam, and Margaret loves apple pie. What does Elizabeth love?\n",
      "\n",
      "'strawberry jam' in statement: tokens [2, 3]\n",
      "  Token count: 2\n",
      "  First token only (token_idx=0): [2]\n"
     ]
    }
   ],
   "source": [
    "# Create example with multi-token entity\n",
    "multi_token_sample = {\n",
    "    \"entity_g0_e0\": \"Elizabeth\",\n",
    "    \"entity_g0_e1\": \"strawberry jam\",\n",
    "    \"entity_g1_e0\": \"Margaret\",\n",
    "    \"entity_g1_e1\": \"apple pie\",\n",
    "    \"query_group\": 0,\n",
    "    \"query_indices\": (0,),\n",
    "    \"answer_index\": 1,\n",
    "    \"active_groups\": 2,\n",
    "    \"entities_per_group\": 2,\n",
    "    \"raw_input\": \"Elizabeth loves strawberry jam, and Margaret loves apple pie. What does Elizabeth love?\",\n",
    "}\n",
    "\n",
    "print(\"Multi-token example:\")\n",
    "print(f\"Prompt: {multi_token_sample['raw_input']}\\n\")\n",
    "\n",
    "# Find multi-token entity in statement\n",
    "jam_tokens = get_entity_token_indices_structured(\n",
    "    multi_token_sample, pipeline, config, 0, 1, region=\"statement\"\n",
    ")\n",
    "print(f\"'strawberry jam' in statement: tokens {jam_tokens}\")\n",
    "print(f\"  Token count: {len(jam_tokens)}\")\n",
    "\n",
    "# Get specific token within multi-token entity\n",
    "if len(jam_tokens) > 1:\n",
    "    first_token = get_entity_token_indices_structured(\n",
    "        multi_token_sample, pipeline, config, 0, 1, region=\"statement\", token_idx=0\n",
    "    )\n",
    "    print(f\"  First token only (token_idx=0): {first_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Action Tasks (3-Entity Groups)\n",
    "\n",
    "The system works with more complex templates too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action task example (person, object, location):\n",
      "Prompt: Kate put watch in the box, Dan put coin in the basket, and Pete put key in the pocket. Where was watch put?\n",
      "Expected answer: box\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create action configuration\n",
    "action_config = create_sample_action_config()\n",
    "action_model = create_direct_causal_model(action_config)\n",
    "\n",
    "# Generate action example\n",
    "action_sample = sample_valid_entity_binding_input(action_config)\n",
    "action_output = action_model.run_forward(action_sample)\n",
    "action_sample[\"raw_input\"] = action_output[\"raw_input\"]\n",
    "\n",
    "print(\"Action task example (person, object, location):\")\n",
    "print(f\"Prompt: {action_sample['raw_input']}\")\n",
    "print(f\"Expected answer: {action_output['raw_output']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action task structure:\n",
      "  21 segments\n",
      "  Statement region: chars (0, 86)\n",
      "  Question region: chars (87, 107)\n",
      "\n",
      "Entities in query group 0 (statement):\n",
      "  Person     ('Kate'): tokens [0]\n",
      "  Object     ('watch'): tokens [2]\n",
      "  Location   ('box'): tokens [5]\n"
     ]
    }
   ],
   "source": [
    "# Parse action task structure\n",
    "action_parser = PromptParser(action_config)\n",
    "action_parsed = action_parser.parse_prompt(action_sample)\n",
    "action_tokenizer = PromptTokenizer(pipeline)\n",
    "action_parsed = action_tokenizer.tokenize_prompt(action_parsed)\n",
    "\n",
    "print(\"Action task structure:\")\n",
    "print(f\"  {len(action_parsed.segments)} segments\")\n",
    "print(f\"  Statement region: chars {action_parsed.statement_region}\")\n",
    "print(f\"  Question region: chars {action_parsed.question_region}\\n\")\n",
    "\n",
    "# Show entities in statement\n",
    "query_group = action_sample[\"query_group\"]\n",
    "print(f\"Entities in query group {query_group} (statement):\")\n",
    "\n",
    "for e in range(action_config.max_entities_per_group):\n",
    "    entity_key = f\"entity_g{query_group}_e{e}\"\n",
    "    entity = action_sample.get(entity_key)\n",
    "\n",
    "    if entity is not None:\n",
    "        role = action_config.entity_roles[e]\n",
    "        tokens = get_entity_token_indices_structured(\n",
    "            action_sample, pipeline, action_config, query_group, e, region=\"statement\"\n",
    "        )\n",
    "        print(f\"  {role.capitalize():10} ('{entity}'): tokens {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Rich Structural Information\n",
    "\n",
    "Beyond just finding entities, the parsed structure gives us rich metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying the parsed structure:\n",
      "\n",
      "Total entity segments: 5\n",
      "Total delimiter segments: 2\n",
      "  Delimiter: ', and ' at chars 14-20\n",
      "  Delimiter: '.' at chars 33-34\n",
      "\n",
      "Pete segments:\n",
      "  'Pete' in statement: tokens [0, 1]\n",
      "  'Pete' in question: tokens [12]\n"
     ]
    }
   ],
   "source": [
    "# Query the parsed structure\n",
    "print(\"Querying the parsed structure:\\n\")\n",
    "\n",
    "# Find all entity segments\n",
    "entity_segments = [s for s in parsed.segments if s.segment_type == \"entity\"]\n",
    "print(f\"Total entity segments: {len(entity_segments)}\")\n",
    "\n",
    "# Find all delimiters\n",
    "delimiter_segments = [s for s in parsed.segments if s.segment_type == \"delimiter\"]\n",
    "print(f\"Total delimiter segments: {len(delimiter_segments)}\")\n",
    "for seg in delimiter_segments:\n",
    "    print(f\"  Delimiter: '{seg.text}' at chars {seg.char_start}-{seg.char_end}\")\n",
    "\n",
    "# Get specific entity segments\n",
    "print(\"\\nPete segments:\")\n",
    "pete_segments = parsed.get_entity_segments(0, 0)\n",
    "for seg in pete_segments:\n",
    "    region = \"statement\" if seg.char_start < parsed.statement_region[1] else \"question\"\n",
    "    print(f\"  '{seg.text}' in {region}: tokens {seg.token_positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Multiple Random Examples\n",
    "\n",
    "Let's see the system work across different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5 random examples with region-aware token positions:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "Prompt: Kate loves tea, Tim loves jam, and Ann loves pie. What does Ann love?\n",
      "Expected answer: pie\n",
      "  Query entity in statement: tokens [9]\n",
      "  Query entity in question: tokens [15]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Prompt: Kate loves soup, Tim loves tea, and Sue loves cake. What does Sue love?\n",
      "Expected answer: cake\n",
      "  Query entity in statement: tokens [9]\n",
      "  Query entity in question: tokens [15]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Prompt: Bob loves pie, and Sue loves bread. What does Bob love?\n",
      "Expected answer: pie\n",
      "  Query entity in statement: tokens [0]\n",
      "  Query entity in question: tokens [11]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Prompt: Sue loves jam, Pete loves pie, and Tim loves soup. What does Sue love?\n",
      "Expected answer: jam\n",
      "  Query entity in statement: tokens [0, 1]\n",
      "  Query entity in question: tokens [16]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Prompt: Bob loves tea, and Sue loves bread. What does Bob love?\n",
      "Expected answer: tea\n",
      "  Query entity in statement: tokens [0]\n",
      "  Query entity in question: tokens [11]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating 5 random examples with region-aware token positions:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model = create_direct_causal_model(config)\n",
    "\n",
    "for i in range(5):\n",
    "    # Generate new sample\n",
    "    sample = sample_valid_entity_binding_input(config)\n",
    "    output = model.run_forward(sample)\n",
    "    sample[\"raw_input\"] = output[\"raw_input\"]\n",
    "\n",
    "    print(f\"\\nExample {i + 1}:\")\n",
    "    print(f\"Prompt: {sample['raw_input']}\")\n",
    "    print(f\"Expected answer: {output['raw_output']}\")\n",
    "\n",
    "    # Get query entity from statement\n",
    "    query_group = sample[\"query_group\"]\n",
    "    query_idx = sample[\"query_indices\"][0]\n",
    "\n",
    "    try:\n",
    "        statement_tokens = get_entity_token_indices_structured(\n",
    "            sample, pipeline, config, query_group, query_idx, region=\"statement\"\n",
    "        )\n",
    "        print(f\"  Query entity in statement: tokens {statement_tokens}\")\n",
    "    except ValueError:\n",
    "        print(\"  Query entity NOT in statement\")\n",
    "\n",
    "    try:\n",
    "        question_tokens = get_entity_token_indices_structured(\n",
    "            sample, pipeline, config, query_group, query_idx, region=\"question\"\n",
    "        )\n",
    "        print(f\"  Query entity in question: tokens {question_tokens}\")\n",
    "    except ValueError:\n",
    "        print(\"  Query entity NOT in question\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Error Handling\n",
    "\n",
    "The system provides clear errors when entities aren't found in the specified region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example without question:\n",
      "Prompt: Pete loves jam.\n",
      "\n",
      "Pete in statement: [0, 1]\n",
      "\n",
      "✓ Expected error: Entity g0_e0 not found in question region\n",
      "  The system correctly reports that there's no question region!\n"
     ]
    }
   ],
   "source": [
    "# Try to get an entity from the wrong region\n",
    "simple_sample = {\n",
    "    \"entity_g0_e0\": \"Pete\",\n",
    "    \"entity_g0_e1\": \"jam\",\n",
    "    \"active_groups\": 1,\n",
    "    \"entities_per_group\": 2,\n",
    "    \"raw_input\": \"Pete loves jam.\",  # No question!\n",
    "}\n",
    "\n",
    "print(\"Example without question:\")\n",
    "print(f\"Prompt: {simple_sample['raw_input']}\\n\")\n",
    "\n",
    "# This works - Pete is in the statement\n",
    "statement_tokens = get_entity_token_indices_structured(\n",
    "    simple_sample, pipeline, config, 0, 0, region=\"statement\"\n",
    ")\n",
    "print(f\"Pete in statement: {statement_tokens}\")\n",
    "\n",
    "# This raises an error - no question region\n",
    "try:\n",
    "    question_tokens = get_entity_token_indices_structured(\n",
    "        simple_sample, pipeline, config, 0, 0, region=\"question\"\n",
    "    )\n",
    "    print(f\"Pete in question: {question_tokens}\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\n✓ Expected error: {e}\")\n",
    "    print(\"  The system correctly reports that there's no question region!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Advantages of Template-Aware Token Positions\n",
    "\n",
    "### Key Improvements:\n",
    "\n",
    "1. **✓ Unambiguous**: Explicitly specify `region='statement'` or `region='question'`\n",
    "2. **✓ Correct**: No more finding wrong occurrence of duplicate entities\n",
    "3. **✓ Rich metadata**: Full structural information (segments, regions, types)\n",
    "4. **✓ Template-aware**: Uses actual template structure, not blind substring matching\n",
    "5. **✓ Extensible**: Easy to add new segment types or query capabilities\n",
    "\n",
    "### API Functions:\n",
    "\n",
    "- **`get_entity_token_indices_structured`**: Get token positions with region specification\n",
    "- **`PromptParser`**: Parse prompts into structured segments\n",
    "- **`PromptTokenizer`**: Add token position information to segments\n",
    "- **`ParsedPrompt`**: Rich data structure with regions and segments\n",
    "\n",
    "### For Intervention Experiments:\n",
    "\n",
    "```python\n",
    "# Get entity from statement (for testing representation)\n",
    "statement_tokens = get_entity_token_indices_structured(\n",
    "    sample, pipeline, config, group_idx, entity_idx, region='statement'\n",
    ")\n",
    "\n",
    "# Get entity from question (for testing retrieval)\n",
    "question_tokens = get_entity_token_indices_structured(\n",
    "    sample, pipeline, config, group_idx, entity_idx, region='question'\n",
    ")\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Use region-specific token positions in intervention experiments\n",
    "- Test how interventions at statement vs question positions differ\n",
    "- Build precise causal abstraction claims about entity binding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
