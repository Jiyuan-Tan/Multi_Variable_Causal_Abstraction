{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **DEPRECATED:** This task is outdated and may not reflect current best practices.\n> See `causalab/tasks/MCQA/` for an up-to-date example.\n\n# Token Positions for Addition: Llama vs Gemma\n\nThis notebook demonstrates how different language models tokenize numbers differently, and why this matters for causal interventions.\n\n## The Critical Question\n\nWhen we want to intervene on \"the tens digit of 23\", which tokens do we intervene on?\n\nThe answer is **model-dependent**:\n- **Llama 3.1 8B**: Tokenizes \"23\" as a **single token** → we intervene on the entire number\n- **Gemma 2 9B**: Tokenizes \"23\" as **two tokens** (\"2\" and \"3\") → we can intervene on individual digits\n\nThis notebook shows these differences concretely and explains their implications for experiments."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import torch\n",
    "from causalab.neural.pipeline import LMPipeline\n",
    "from causalab.tasks.general_addition.config import (\n",
    "    create_two_number_two_digit_config,\n",
    "    create_two_number_three_digit_config,\n",
    "    create_general_config,\n",
    ")\n",
    "from causalab.tasks.general_addition.causal_models import (\n",
    "    create_basic_addition_model,\n",
    "    sample_valid_addition_input,\n",
    ")\n",
    "from causalab.tasks.general_addition.token_positions import (\n",
    "    get_digit_token_position,\n",
    "    create_token_positions,\n",
    ")\n",
    "from causalab.tasks.general_addition.experiments.tokenization_config import (\n",
    "    get_tokens_per_number,\n",
    "    TOKENIZATION_CONFIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading the Models\n",
    "\n",
    "We'll load pipeline objects for both Llama 3.1 8B and Gemma 2 9B. The pipeline includes the tokenizer, which is what we need to see how each model breaks text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama 3.1 8B pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39f5cf0a23d40149914d23c1040db45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Gemma 2 9B pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c057901ecb74b18b70319f1c28778ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Both models ready!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Loading Llama 3.1 8B pipeline...\")\n",
    "llama_pipeline = LMPipeline(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_new_tokens=5,\n",
    "    device=device,\n",
    "    dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "print(\"\\nLoading Gemma 2 9B pipeline...\")\n",
    "gemma_pipeline = LMPipeline(\n",
    "    \"google/gemma-2-9b\",\n",
    "    max_new_tokens=5,\n",
    "    device=device,\n",
    "    dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Both models ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create an Example Prompt (2-Digit Addition)\n",
    "\n",
    "Let's create a simple addition problem: 23 + 45 = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example prompt: 'The sum of 23 and 45 is'\n",
      "Expected answer: '68'\n",
      "\n",
      "Breakdown: We have two numbers: '23' and '45'\n",
      "  - digit_0_0 = 2 (tens place of first number)\n",
      "  - digit_0_1 = 3 (ones place of first number)\n",
      "  - digit_1_0 = 4 (tens place of second number)\n",
      "  - digit_1_1 = 5 (ones place of second number)\n"
     ]
    }
   ],
   "source": [
    "# Create configuration and model\n",
    "config = create_two_number_two_digit_config()\n",
    "model = create_basic_addition_model(config)\n",
    "\n",
    "# Create a specific example: 23 + 45\n",
    "input_sample = {\n",
    "    \"digit_0_0\": 2,\n",
    "    \"digit_0_1\": 3,  # 23\n",
    "    \"digit_1_0\": 4,\n",
    "    \"digit_1_1\": 5,  # 45\n",
    "    \"num_addends\": 2,\n",
    "    \"num_digits\": 2,\n",
    "    \"template\": config.templates[0],\n",
    "}\n",
    "\n",
    "# Generate the prompt\n",
    "output = model.new_trace(input_sample)\n",
    "prompt = output['raw_input']\n",
    "input_sample[\"raw_input\"] = output['raw_input']\n",
    "\n",
    "print(f\"Example prompt: '{prompt}'\")\n",
    "print(f\"Expected answer: '{output['raw_output'].strip()}'\")\n",
    "print(\"\\nBreakdown: We have two numbers: '23' and '45'\")\n",
    "print(\"  - digit_0_0 = 2 (tens place of first number)\")\n",
    "print(\"  - digit_0_1 = 3 (ones place of first number)\")\n",
    "print(\"  - digit_1_0 = 4 (tens place of second number)\")\n",
    "print(\"  - digit_1_1 = 5 (ones place of second number)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Llama 3.1 8B Tokenization\n",
    "\n",
    "Let's see how Llama tokenizes this prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3.1 8B Tokenization:\n",
      "======================================================================\n",
      "Total tokens: 256\n",
      "\n",
      "Token 0: '<|eot_id|>'         (ID: 128009)\n",
      "Token 1: '<|eot_id|>'         (ID: 128009)\n",
      "Token 2: '<|eot_id|>'         (ID: 128009)\n",
      "Token 3: '<|eot_id|>'         (ID: 128009)\n",
      "Token 4: '<|eot_id|>'         (ID: 128009)\n",
      "Token 5: '<|eot_id|>'         (ID: 128009)\n",
      "Token 6: '<|eot_id|>'         (ID: 128009)\n",
      "Token 7: '<|eot_id|>'         (ID: 128009)\n",
      "Token 8: '<|eot_id|>'         (ID: 128009)\n",
      "Token 9: '<|eot_id|>'         (ID: 128009)\n",
      "Token 10: '<|eot_id|>'         (ID: 128009)\n",
      "Token 11: '<|eot_id|>'         (ID: 128009)\n",
      "Token 12: '<|eot_id|>'         (ID: 128009)\n",
      "Token 13: '<|eot_id|>'         (ID: 128009)\n",
      "Token 14: '<|eot_id|>'         (ID: 128009)\n",
      "Token 15: '<|eot_id|>'         (ID: 128009)\n",
      "Token 16: '<|eot_id|>'         (ID: 128009)\n",
      "Token 17: '<|eot_id|>'         (ID: 128009)\n",
      "Token 18: '<|eot_id|>'         (ID: 128009)\n",
      "Token 19: '<|eot_id|>'         (ID: 128009)\n",
      "Token 20: '<|eot_id|>'         (ID: 128009)\n",
      "Token 21: '<|eot_id|>'         (ID: 128009)\n",
      "Token 22: '<|eot_id|>'         (ID: 128009)\n",
      "Token 23: '<|eot_id|>'         (ID: 128009)\n",
      "Token 24: '<|eot_id|>'         (ID: 128009)\n",
      "Token 25: '<|eot_id|>'         (ID: 128009)\n",
      "Token 26: '<|eot_id|>'         (ID: 128009)\n",
      "Token 27: '<|eot_id|>'         (ID: 128009)\n",
      "Token 28: '<|eot_id|>'         (ID: 128009)\n",
      "Token 29: '<|eot_id|>'         (ID: 128009)\n",
      "Token 30: '<|eot_id|>'         (ID: 128009)\n",
      "Token 31: '<|eot_id|>'         (ID: 128009)\n",
      "Token 32: '<|eot_id|>'         (ID: 128009)\n",
      "Token 33: '<|eot_id|>'         (ID: 128009)\n",
      "Token 34: '<|eot_id|>'         (ID: 128009)\n",
      "Token 35: '<|eot_id|>'         (ID: 128009)\n",
      "Token 36: '<|eot_id|>'         (ID: 128009)\n",
      "Token 37: '<|eot_id|>'         (ID: 128009)\n",
      "Token 38: '<|eot_id|>'         (ID: 128009)\n",
      "Token 39: '<|eot_id|>'         (ID: 128009)\n",
      "Token 40: '<|eot_id|>'         (ID: 128009)\n",
      "Token 41: '<|eot_id|>'         (ID: 128009)\n",
      "Token 42: '<|eot_id|>'         (ID: 128009)\n",
      "Token 43: '<|eot_id|>'         (ID: 128009)\n",
      "Token 44: '<|eot_id|>'         (ID: 128009)\n",
      "Token 45: '<|eot_id|>'         (ID: 128009)\n",
      "Token 46: '<|eot_id|>'         (ID: 128009)\n",
      "Token 47: '<|eot_id|>'         (ID: 128009)\n",
      "Token 48: '<|eot_id|>'         (ID: 128009)\n",
      "Token 49: '<|eot_id|>'         (ID: 128009)\n",
      "Token 50: '<|eot_id|>'         (ID: 128009)\n",
      "Token 51: '<|eot_id|>'         (ID: 128009)\n",
      "Token 52: '<|eot_id|>'         (ID: 128009)\n",
      "Token 53: '<|eot_id|>'         (ID: 128009)\n",
      "Token 54: '<|eot_id|>'         (ID: 128009)\n",
      "Token 55: '<|eot_id|>'         (ID: 128009)\n",
      "Token 56: '<|eot_id|>'         (ID: 128009)\n",
      "Token 57: '<|eot_id|>'         (ID: 128009)\n",
      "Token 58: '<|eot_id|>'         (ID: 128009)\n",
      "Token 59: '<|eot_id|>'         (ID: 128009)\n",
      "Token 60: '<|eot_id|>'         (ID: 128009)\n",
      "Token 61: '<|eot_id|>'         (ID: 128009)\n",
      "Token 62: '<|eot_id|>'         (ID: 128009)\n",
      "Token 63: '<|eot_id|>'         (ID: 128009)\n",
      "Token 64: '<|eot_id|>'         (ID: 128009)\n",
      "Token 65: '<|eot_id|>'         (ID: 128009)\n",
      "Token 66: '<|eot_id|>'         (ID: 128009)\n",
      "Token 67: '<|eot_id|>'         (ID: 128009)\n",
      "Token 68: '<|eot_id|>'         (ID: 128009)\n",
      "Token 69: '<|eot_id|>'         (ID: 128009)\n",
      "Token 70: '<|eot_id|>'         (ID: 128009)\n",
      "Token 71: '<|eot_id|>'         (ID: 128009)\n",
      "Token 72: '<|eot_id|>'         (ID: 128009)\n",
      "Token 73: '<|eot_id|>'         (ID: 128009)\n",
      "Token 74: '<|eot_id|>'         (ID: 128009)\n",
      "Token 75: '<|eot_id|>'         (ID: 128009)\n",
      "Token 76: '<|eot_id|>'         (ID: 128009)\n",
      "Token 77: '<|eot_id|>'         (ID: 128009)\n",
      "Token 78: '<|eot_id|>'         (ID: 128009)\n",
      "Token 79: '<|eot_id|>'         (ID: 128009)\n",
      "Token 80: '<|eot_id|>'         (ID: 128009)\n",
      "Token 81: '<|eot_id|>'         (ID: 128009)\n",
      "Token 82: '<|eot_id|>'         (ID: 128009)\n",
      "Token 83: '<|eot_id|>'         (ID: 128009)\n",
      "Token 84: '<|eot_id|>'         (ID: 128009)\n",
      "Token 85: '<|eot_id|>'         (ID: 128009)\n",
      "Token 86: '<|eot_id|>'         (ID: 128009)\n",
      "Token 87: '<|eot_id|>'         (ID: 128009)\n",
      "Token 88: '<|eot_id|>'         (ID: 128009)\n",
      "Token 89: '<|eot_id|>'         (ID: 128009)\n",
      "Token 90: '<|eot_id|>'         (ID: 128009)\n",
      "Token 91: '<|eot_id|>'         (ID: 128009)\n",
      "Token 92: '<|eot_id|>'         (ID: 128009)\n",
      "Token 93: '<|eot_id|>'         (ID: 128009)\n",
      "Token 94: '<|eot_id|>'         (ID: 128009)\n",
      "Token 95: '<|eot_id|>'         (ID: 128009)\n",
      "Token 96: '<|eot_id|>'         (ID: 128009)\n",
      "Token 97: '<|eot_id|>'         (ID: 128009)\n",
      "Token 98: '<|eot_id|>'         (ID: 128009)\n",
      "Token 99: '<|eot_id|>'         (ID: 128009)\n",
      "Token 100: '<|eot_id|>'         (ID: 128009)\n",
      "Token 101: '<|eot_id|>'         (ID: 128009)\n",
      "Token 102: '<|eot_id|>'         (ID: 128009)\n",
      "Token 103: '<|eot_id|>'         (ID: 128009)\n",
      "Token 104: '<|eot_id|>'         (ID: 128009)\n",
      "Token 105: '<|eot_id|>'         (ID: 128009)\n",
      "Token 106: '<|eot_id|>'         (ID: 128009)\n",
      "Token 107: '<|eot_id|>'         (ID: 128009)\n",
      "Token 108: '<|eot_id|>'         (ID: 128009)\n",
      "Token 109: '<|eot_id|>'         (ID: 128009)\n",
      "Token 110: '<|eot_id|>'         (ID: 128009)\n",
      "Token 111: '<|eot_id|>'         (ID: 128009)\n",
      "Token 112: '<|eot_id|>'         (ID: 128009)\n",
      "Token 113: '<|eot_id|>'         (ID: 128009)\n",
      "Token 114: '<|eot_id|>'         (ID: 128009)\n",
      "Token 115: '<|eot_id|>'         (ID: 128009)\n",
      "Token 116: '<|eot_id|>'         (ID: 128009)\n",
      "Token 117: '<|eot_id|>'         (ID: 128009)\n",
      "Token 118: '<|eot_id|>'         (ID: 128009)\n",
      "Token 119: '<|eot_id|>'         (ID: 128009)\n",
      "Token 120: '<|eot_id|>'         (ID: 128009)\n",
      "Token 121: '<|eot_id|>'         (ID: 128009)\n",
      "Token 122: '<|eot_id|>'         (ID: 128009)\n",
      "Token 123: '<|eot_id|>'         (ID: 128009)\n",
      "Token 124: '<|eot_id|>'         (ID: 128009)\n",
      "Token 125: '<|eot_id|>'         (ID: 128009)\n",
      "Token 126: '<|eot_id|>'         (ID: 128009)\n",
      "Token 127: '<|eot_id|>'         (ID: 128009)\n",
      "Token 128: '<|eot_id|>'         (ID: 128009)\n",
      "Token 129: '<|eot_id|>'         (ID: 128009)\n",
      "Token 130: '<|eot_id|>'         (ID: 128009)\n",
      "Token 131: '<|eot_id|>'         (ID: 128009)\n",
      "Token 132: '<|eot_id|>'         (ID: 128009)\n",
      "Token 133: '<|eot_id|>'         (ID: 128009)\n",
      "Token 134: '<|eot_id|>'         (ID: 128009)\n",
      "Token 135: '<|eot_id|>'         (ID: 128009)\n",
      "Token 136: '<|eot_id|>'         (ID: 128009)\n",
      "Token 137: '<|eot_id|>'         (ID: 128009)\n",
      "Token 138: '<|eot_id|>'         (ID: 128009)\n",
      "Token 139: '<|eot_id|>'         (ID: 128009)\n",
      "Token 140: '<|eot_id|>'         (ID: 128009)\n",
      "Token 141: '<|eot_id|>'         (ID: 128009)\n",
      "Token 142: '<|eot_id|>'         (ID: 128009)\n",
      "Token 143: '<|eot_id|>'         (ID: 128009)\n",
      "Token 144: '<|eot_id|>'         (ID: 128009)\n",
      "Token 145: '<|eot_id|>'         (ID: 128009)\n",
      "Token 146: '<|eot_id|>'         (ID: 128009)\n",
      "Token 147: '<|eot_id|>'         (ID: 128009)\n",
      "Token 148: '<|eot_id|>'         (ID: 128009)\n",
      "Token 149: '<|eot_id|>'         (ID: 128009)\n",
      "Token 150: '<|eot_id|>'         (ID: 128009)\n",
      "Token 151: '<|eot_id|>'         (ID: 128009)\n",
      "Token 152: '<|eot_id|>'         (ID: 128009)\n",
      "Token 153: '<|eot_id|>'         (ID: 128009)\n",
      "Token 154: '<|eot_id|>'         (ID: 128009)\n",
      "Token 155: '<|eot_id|>'         (ID: 128009)\n",
      "Token 156: '<|eot_id|>'         (ID: 128009)\n",
      "Token 157: '<|eot_id|>'         (ID: 128009)\n",
      "Token 158: '<|eot_id|>'         (ID: 128009)\n",
      "Token 159: '<|eot_id|>'         (ID: 128009)\n",
      "Token 160: '<|eot_id|>'         (ID: 128009)\n",
      "Token 161: '<|eot_id|>'         (ID: 128009)\n",
      "Token 162: '<|eot_id|>'         (ID: 128009)\n",
      "Token 163: '<|eot_id|>'         (ID: 128009)\n",
      "Token 164: '<|eot_id|>'         (ID: 128009)\n",
      "Token 165: '<|eot_id|>'         (ID: 128009)\n",
      "Token 166: '<|eot_id|>'         (ID: 128009)\n",
      "Token 167: '<|eot_id|>'         (ID: 128009)\n",
      "Token 168: '<|eot_id|>'         (ID: 128009)\n",
      "Token 169: '<|eot_id|>'         (ID: 128009)\n",
      "Token 170: '<|eot_id|>'         (ID: 128009)\n",
      "Token 171: '<|eot_id|>'         (ID: 128009)\n",
      "Token 172: '<|eot_id|>'         (ID: 128009)\n",
      "Token 173: '<|eot_id|>'         (ID: 128009)\n",
      "Token 174: '<|eot_id|>'         (ID: 128009)\n",
      "Token 175: '<|eot_id|>'         (ID: 128009)\n",
      "Token 176: '<|eot_id|>'         (ID: 128009)\n",
      "Token 177: '<|eot_id|>'         (ID: 128009)\n",
      "Token 178: '<|eot_id|>'         (ID: 128009)\n",
      "Token 179: '<|eot_id|>'         (ID: 128009)\n",
      "Token 180: '<|eot_id|>'         (ID: 128009)\n",
      "Token 181: '<|eot_id|>'         (ID: 128009)\n",
      "Token 182: '<|eot_id|>'         (ID: 128009)\n",
      "Token 183: '<|eot_id|>'         (ID: 128009)\n",
      "Token 184: '<|eot_id|>'         (ID: 128009)\n",
      "Token 185: '<|eot_id|>'         (ID: 128009)\n",
      "Token 186: '<|eot_id|>'         (ID: 128009)\n",
      "Token 187: '<|eot_id|>'         (ID: 128009)\n",
      "Token 188: '<|eot_id|>'         (ID: 128009)\n",
      "Token 189: '<|eot_id|>'         (ID: 128009)\n",
      "Token 190: '<|eot_id|>'         (ID: 128009)\n",
      "Token 191: '<|eot_id|>'         (ID: 128009)\n",
      "Token 192: '<|eot_id|>'         (ID: 128009)\n",
      "Token 193: '<|eot_id|>'         (ID: 128009)\n",
      "Token 194: '<|eot_id|>'         (ID: 128009)\n",
      "Token 195: '<|eot_id|>'         (ID: 128009)\n",
      "Token 196: '<|eot_id|>'         (ID: 128009)\n",
      "Token 197: '<|eot_id|>'         (ID: 128009)\n",
      "Token 198: '<|eot_id|>'         (ID: 128009)\n",
      "Token 199: '<|eot_id|>'         (ID: 128009)\n",
      "Token 200: '<|eot_id|>'         (ID: 128009)\n",
      "Token 201: '<|eot_id|>'         (ID: 128009)\n",
      "Token 202: '<|eot_id|>'         (ID: 128009)\n",
      "Token 203: '<|eot_id|>'         (ID: 128009)\n",
      "Token 204: '<|eot_id|>'         (ID: 128009)\n",
      "Token 205: '<|eot_id|>'         (ID: 128009)\n",
      "Token 206: '<|eot_id|>'         (ID: 128009)\n",
      "Token 207: '<|eot_id|>'         (ID: 128009)\n",
      "Token 208: '<|eot_id|>'         (ID: 128009)\n",
      "Token 209: '<|eot_id|>'         (ID: 128009)\n",
      "Token 210: '<|eot_id|>'         (ID: 128009)\n",
      "Token 211: '<|eot_id|>'         (ID: 128009)\n",
      "Token 212: '<|eot_id|>'         (ID: 128009)\n",
      "Token 213: '<|eot_id|>'         (ID: 128009)\n",
      "Token 214: '<|eot_id|>'         (ID: 128009)\n",
      "Token 215: '<|eot_id|>'         (ID: 128009)\n",
      "Token 216: '<|eot_id|>'         (ID: 128009)\n",
      "Token 217: '<|eot_id|>'         (ID: 128009)\n",
      "Token 218: '<|eot_id|>'         (ID: 128009)\n",
      "Token 219: '<|eot_id|>'         (ID: 128009)\n",
      "Token 220: '<|eot_id|>'         (ID: 128009)\n",
      "Token 221: '<|eot_id|>'         (ID: 128009)\n",
      "Token 222: '<|eot_id|>'         (ID: 128009)\n",
      "Token 223: '<|eot_id|>'         (ID: 128009)\n",
      "Token 224: '<|eot_id|>'         (ID: 128009)\n",
      "Token 225: '<|eot_id|>'         (ID: 128009)\n",
      "Token 226: '<|eot_id|>'         (ID: 128009)\n",
      "Token 227: '<|eot_id|>'         (ID: 128009)\n",
      "Token 228: '<|eot_id|>'         (ID: 128009)\n",
      "Token 229: '<|eot_id|>'         (ID: 128009)\n",
      "Token 230: '<|eot_id|>'         (ID: 128009)\n",
      "Token 231: '<|eot_id|>'         (ID: 128009)\n",
      "Token 232: '<|eot_id|>'         (ID: 128009)\n",
      "Token 233: '<|eot_id|>'         (ID: 128009)\n",
      "Token 234: '<|eot_id|>'         (ID: 128009)\n",
      "Token 235: '<|eot_id|>'         (ID: 128009)\n",
      "Token 236: '<|eot_id|>'         (ID: 128009)\n",
      "Token 237: '<|eot_id|>'         (ID: 128009)\n",
      "Token 238: '<|eot_id|>'         (ID: 128009)\n",
      "Token 239: '<|eot_id|>'         (ID: 128009)\n",
      "Token 240: '<|eot_id|>'         (ID: 128009)\n",
      "Token 241: '<|eot_id|>'         (ID: 128009)\n",
      "Token 242: '<|eot_id|>'         (ID: 128009)\n",
      "Token 243: '<|eot_id|>'         (ID: 128009)\n",
      "Token 244: '<|eot_id|>'         (ID: 128009)\n",
      "Token 245: '<|eot_id|>'         (ID: 128009)\n",
      "Token 246: '<|eot_id|>'         (ID: 128009)\n",
      "Token 247: 'The'                (ID: 791)\n",
      "Token 248: ' sum'               (ID: 2694)\n",
      "Token 249: ' of'                (ID: 315)\n",
      "Token 250: ' '                  (ID: 220)\n",
      "Token 251: '23'                 (ID: 1419) <-- FIRST NUMBER (entire '23')\n",
      "Token 252: ' and'               (ID: 323)\n",
      "Token 253: ' '                  (ID: 220)\n",
      "Token 254: '45'                 (ID: 1774) <-- SECOND NUMBER (entire '45')\n",
      "Token 255: ' is'                (ID: 374)\n",
      "\n",
      "======================================================================\n",
      "KEY OBSERVATION: '23' and '45' are each encoded as SINGLE TOKENS\n",
      "This means we cannot intervene on individual digits!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with Llama using pipeline.load() for consistency with get_digit_token_position\n",
    "llama_token_dict = llama_pipeline.load({\"raw_input\": prompt}, add_special_tokens=False)\n",
    "llama_tokens = llama_token_dict[\"input_ids\"][0].tolist()\n",
    "\n",
    "print(\"Llama 3.1 8B Tokenization:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total tokens: {len(llama_tokens)}\\n\")\n",
    "\n",
    "for i, token_id in enumerate(llama_tokens):\n",
    "    token_str = llama_pipeline.tokenizer.decode([token_id])\n",
    "    marker = \"\"\n",
    "    if \"23\" in token_str:\n",
    "        marker = \" <-- FIRST NUMBER (entire '23')\"\n",
    "    elif \"45\" in token_str:\n",
    "        marker = \" <-- SECOND NUMBER (entire '45')\"\n",
    "    print(f\"Token {i}: {repr(token_str):20s} (ID: {token_id}){marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY OBSERVATION: '23' and '45' are each encoded as SINGLE TOKENS\")\n",
    "print(\"This means we cannot intervene on individual digits!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Gemma 2 9B Tokenization\n",
    "\n",
    "Now let's see how Gemma tokenizes the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma 2 9B Tokenization:\n",
      "======================================================================\n",
      "Total tokens: 256\n",
      "\n",
      "Token 0: '<eos>'              (ID: 1)\n",
      "Token 1: '<eos>'              (ID: 1)\n",
      "Token 2: '<eos>'              (ID: 1)\n",
      "Token 3: '<eos>'              (ID: 1)\n",
      "Token 4: '<eos>'              (ID: 1)\n",
      "Token 5: '<eos>'              (ID: 1)\n",
      "Token 6: '<eos>'              (ID: 1)\n",
      "Token 7: '<eos>'              (ID: 1)\n",
      "Token 8: '<eos>'              (ID: 1)\n",
      "Token 9: '<eos>'              (ID: 1)\n",
      "Token 10: '<eos>'              (ID: 1)\n",
      "Token 11: '<eos>'              (ID: 1)\n",
      "Token 12: '<eos>'              (ID: 1)\n",
      "Token 13: '<eos>'              (ID: 1)\n",
      "Token 14: '<eos>'              (ID: 1)\n",
      "Token 15: '<eos>'              (ID: 1)\n",
      "Token 16: '<eos>'              (ID: 1)\n",
      "Token 17: '<eos>'              (ID: 1)\n",
      "Token 18: '<eos>'              (ID: 1)\n",
      "Token 19: '<eos>'              (ID: 1)\n",
      "Token 20: '<eos>'              (ID: 1)\n",
      "Token 21: '<eos>'              (ID: 1)\n",
      "Token 22: '<eos>'              (ID: 1)\n",
      "Token 23: '<eos>'              (ID: 1)\n",
      "Token 24: '<eos>'              (ID: 1)\n",
      "Token 25: '<eos>'              (ID: 1)\n",
      "Token 26: '<eos>'              (ID: 1)\n",
      "Token 27: '<eos>'              (ID: 1)\n",
      "Token 28: '<eos>'              (ID: 1)\n",
      "Token 29: '<eos>'              (ID: 1)\n",
      "Token 30: '<eos>'              (ID: 1)\n",
      "Token 31: '<eos>'              (ID: 1)\n",
      "Token 32: '<eos>'              (ID: 1)\n",
      "Token 33: '<eos>'              (ID: 1)\n",
      "Token 34: '<eos>'              (ID: 1)\n",
      "Token 35: '<eos>'              (ID: 1)\n",
      "Token 36: '<eos>'              (ID: 1)\n",
      "Token 37: '<eos>'              (ID: 1)\n",
      "Token 38: '<eos>'              (ID: 1)\n",
      "Token 39: '<eos>'              (ID: 1)\n",
      "Token 40: '<eos>'              (ID: 1)\n",
      "Token 41: '<eos>'              (ID: 1)\n",
      "Token 42: '<eos>'              (ID: 1)\n",
      "Token 43: '<eos>'              (ID: 1)\n",
      "Token 44: '<eos>'              (ID: 1)\n",
      "Token 45: '<eos>'              (ID: 1)\n",
      "Token 46: '<eos>'              (ID: 1)\n",
      "Token 47: '<eos>'              (ID: 1)\n",
      "Token 48: '<eos>'              (ID: 1)\n",
      "Token 49: '<eos>'              (ID: 1)\n",
      "Token 50: '<eos>'              (ID: 1)\n",
      "Token 51: '<eos>'              (ID: 1)\n",
      "Token 52: '<eos>'              (ID: 1)\n",
      "Token 53: '<eos>'              (ID: 1)\n",
      "Token 54: '<eos>'              (ID: 1)\n",
      "Token 55: '<eos>'              (ID: 1)\n",
      "Token 56: '<eos>'              (ID: 1)\n",
      "Token 57: '<eos>'              (ID: 1)\n",
      "Token 58: '<eos>'              (ID: 1)\n",
      "Token 59: '<eos>'              (ID: 1)\n",
      "Token 60: '<eos>'              (ID: 1)\n",
      "Token 61: '<eos>'              (ID: 1)\n",
      "Token 62: '<eos>'              (ID: 1)\n",
      "Token 63: '<eos>'              (ID: 1)\n",
      "Token 64: '<eos>'              (ID: 1)\n",
      "Token 65: '<eos>'              (ID: 1)\n",
      "Token 66: '<eos>'              (ID: 1)\n",
      "Token 67: '<eos>'              (ID: 1)\n",
      "Token 68: '<eos>'              (ID: 1)\n",
      "Token 69: '<eos>'              (ID: 1)\n",
      "Token 70: '<eos>'              (ID: 1)\n",
      "Token 71: '<eos>'              (ID: 1)\n",
      "Token 72: '<eos>'              (ID: 1)\n",
      "Token 73: '<eos>'              (ID: 1)\n",
      "Token 74: '<eos>'              (ID: 1)\n",
      "Token 75: '<eos>'              (ID: 1)\n",
      "Token 76: '<eos>'              (ID: 1)\n",
      "Token 77: '<eos>'              (ID: 1)\n",
      "Token 78: '<eos>'              (ID: 1)\n",
      "Token 79: '<eos>'              (ID: 1)\n",
      "Token 80: '<eos>'              (ID: 1)\n",
      "Token 81: '<eos>'              (ID: 1)\n",
      "Token 82: '<eos>'              (ID: 1)\n",
      "Token 83: '<eos>'              (ID: 1)\n",
      "Token 84: '<eos>'              (ID: 1)\n",
      "Token 85: '<eos>'              (ID: 1)\n",
      "Token 86: '<eos>'              (ID: 1)\n",
      "Token 87: '<eos>'              (ID: 1)\n",
      "Token 88: '<eos>'              (ID: 1)\n",
      "Token 89: '<eos>'              (ID: 1)\n",
      "Token 90: '<eos>'              (ID: 1)\n",
      "Token 91: '<eos>'              (ID: 1)\n",
      "Token 92: '<eos>'              (ID: 1)\n",
      "Token 93: '<eos>'              (ID: 1)\n",
      "Token 94: '<eos>'              (ID: 1)\n",
      "Token 95: '<eos>'              (ID: 1)\n",
      "Token 96: '<eos>'              (ID: 1)\n",
      "Token 97: '<eos>'              (ID: 1)\n",
      "Token 98: '<eos>'              (ID: 1)\n",
      "Token 99: '<eos>'              (ID: 1)\n",
      "Token 100: '<eos>'              (ID: 1)\n",
      "Token 101: '<eos>'              (ID: 1)\n",
      "Token 102: '<eos>'              (ID: 1)\n",
      "Token 103: '<eos>'              (ID: 1)\n",
      "Token 104: '<eos>'              (ID: 1)\n",
      "Token 105: '<eos>'              (ID: 1)\n",
      "Token 106: '<eos>'              (ID: 1)\n",
      "Token 107: '<eos>'              (ID: 1)\n",
      "Token 108: '<eos>'              (ID: 1)\n",
      "Token 109: '<eos>'              (ID: 1)\n",
      "Token 110: '<eos>'              (ID: 1)\n",
      "Token 111: '<eos>'              (ID: 1)\n",
      "Token 112: '<eos>'              (ID: 1)\n",
      "Token 113: '<eos>'              (ID: 1)\n",
      "Token 114: '<eos>'              (ID: 1)\n",
      "Token 115: '<eos>'              (ID: 1)\n",
      "Token 116: '<eos>'              (ID: 1)\n",
      "Token 117: '<eos>'              (ID: 1)\n",
      "Token 118: '<eos>'              (ID: 1)\n",
      "Token 119: '<eos>'              (ID: 1)\n",
      "Token 120: '<eos>'              (ID: 1)\n",
      "Token 121: '<eos>'              (ID: 1)\n",
      "Token 122: '<eos>'              (ID: 1)\n",
      "Token 123: '<eos>'              (ID: 1)\n",
      "Token 124: '<eos>'              (ID: 1)\n",
      "Token 125: '<eos>'              (ID: 1)\n",
      "Token 126: '<eos>'              (ID: 1)\n",
      "Token 127: '<eos>'              (ID: 1)\n",
      "Token 128: '<eos>'              (ID: 1)\n",
      "Token 129: '<eos>'              (ID: 1)\n",
      "Token 130: '<eos>'              (ID: 1)\n",
      "Token 131: '<eos>'              (ID: 1)\n",
      "Token 132: '<eos>'              (ID: 1)\n",
      "Token 133: '<eos>'              (ID: 1)\n",
      "Token 134: '<eos>'              (ID: 1)\n",
      "Token 135: '<eos>'              (ID: 1)\n",
      "Token 136: '<eos>'              (ID: 1)\n",
      "Token 137: '<eos>'              (ID: 1)\n",
      "Token 138: '<eos>'              (ID: 1)\n",
      "Token 139: '<eos>'              (ID: 1)\n",
      "Token 140: '<eos>'              (ID: 1)\n",
      "Token 141: '<eos>'              (ID: 1)\n",
      "Token 142: '<eos>'              (ID: 1)\n",
      "Token 143: '<eos>'              (ID: 1)\n",
      "Token 144: '<eos>'              (ID: 1)\n",
      "Token 145: '<eos>'              (ID: 1)\n",
      "Token 146: '<eos>'              (ID: 1)\n",
      "Token 147: '<eos>'              (ID: 1)\n",
      "Token 148: '<eos>'              (ID: 1)\n",
      "Token 149: '<eos>'              (ID: 1)\n",
      "Token 150: '<eos>'              (ID: 1)\n",
      "Token 151: '<eos>'              (ID: 1)\n",
      "Token 152: '<eos>'              (ID: 1)\n",
      "Token 153: '<eos>'              (ID: 1)\n",
      "Token 154: '<eos>'              (ID: 1)\n",
      "Token 155: '<eos>'              (ID: 1)\n",
      "Token 156: '<eos>'              (ID: 1)\n",
      "Token 157: '<eos>'              (ID: 1)\n",
      "Token 158: '<eos>'              (ID: 1)\n",
      "Token 159: '<eos>'              (ID: 1)\n",
      "Token 160: '<eos>'              (ID: 1)\n",
      "Token 161: '<eos>'              (ID: 1)\n",
      "Token 162: '<eos>'              (ID: 1)\n",
      "Token 163: '<eos>'              (ID: 1)\n",
      "Token 164: '<eos>'              (ID: 1)\n",
      "Token 165: '<eos>'              (ID: 1)\n",
      "Token 166: '<eos>'              (ID: 1)\n",
      "Token 167: '<eos>'              (ID: 1)\n",
      "Token 168: '<eos>'              (ID: 1)\n",
      "Token 169: '<eos>'              (ID: 1)\n",
      "Token 170: '<eos>'              (ID: 1)\n",
      "Token 171: '<eos>'              (ID: 1)\n",
      "Token 172: '<eos>'              (ID: 1)\n",
      "Token 173: '<eos>'              (ID: 1)\n",
      "Token 174: '<eos>'              (ID: 1)\n",
      "Token 175: '<eos>'              (ID: 1)\n",
      "Token 176: '<eos>'              (ID: 1)\n",
      "Token 177: '<eos>'              (ID: 1)\n",
      "Token 178: '<eos>'              (ID: 1)\n",
      "Token 179: '<eos>'              (ID: 1)\n",
      "Token 180: '<eos>'              (ID: 1)\n",
      "Token 181: '<eos>'              (ID: 1)\n",
      "Token 182: '<eos>'              (ID: 1)\n",
      "Token 183: '<eos>'              (ID: 1)\n",
      "Token 184: '<eos>'              (ID: 1)\n",
      "Token 185: '<eos>'              (ID: 1)\n",
      "Token 186: '<eos>'              (ID: 1)\n",
      "Token 187: '<eos>'              (ID: 1)\n",
      "Token 188: '<eos>'              (ID: 1)\n",
      "Token 189: '<eos>'              (ID: 1)\n",
      "Token 190: '<eos>'              (ID: 1)\n",
      "Token 191: '<eos>'              (ID: 1)\n",
      "Token 192: '<eos>'              (ID: 1)\n",
      "Token 193: '<eos>'              (ID: 1)\n",
      "Token 194: '<eos>'              (ID: 1)\n",
      "Token 195: '<eos>'              (ID: 1)\n",
      "Token 196: '<eos>'              (ID: 1)\n",
      "Token 197: '<eos>'              (ID: 1)\n",
      "Token 198: '<eos>'              (ID: 1)\n",
      "Token 199: '<eos>'              (ID: 1)\n",
      "Token 200: '<eos>'              (ID: 1)\n",
      "Token 201: '<eos>'              (ID: 1)\n",
      "Token 202: '<eos>'              (ID: 1)\n",
      "Token 203: '<eos>'              (ID: 1)\n",
      "Token 204: '<eos>'              (ID: 1)\n",
      "Token 205: '<eos>'              (ID: 1)\n",
      "Token 206: '<eos>'              (ID: 1)\n",
      "Token 207: '<eos>'              (ID: 1)\n",
      "Token 208: '<eos>'              (ID: 1)\n",
      "Token 209: '<eos>'              (ID: 1)\n",
      "Token 210: '<eos>'              (ID: 1)\n",
      "Token 211: '<eos>'              (ID: 1)\n",
      "Token 212: '<eos>'              (ID: 1)\n",
      "Token 213: '<eos>'              (ID: 1)\n",
      "Token 214: '<eos>'              (ID: 1)\n",
      "Token 215: '<eos>'              (ID: 1)\n",
      "Token 216: '<eos>'              (ID: 1)\n",
      "Token 217: '<eos>'              (ID: 1)\n",
      "Token 218: '<eos>'              (ID: 1)\n",
      "Token 219: '<eos>'              (ID: 1)\n",
      "Token 220: '<eos>'              (ID: 1)\n",
      "Token 221: '<eos>'              (ID: 1)\n",
      "Token 222: '<eos>'              (ID: 1)\n",
      "Token 223: '<eos>'              (ID: 1)\n",
      "Token 224: '<eos>'              (ID: 1)\n",
      "Token 225: '<eos>'              (ID: 1)\n",
      "Token 226: '<eos>'              (ID: 1)\n",
      "Token 227: '<eos>'              (ID: 1)\n",
      "Token 228: '<eos>'              (ID: 1)\n",
      "Token 229: '<eos>'              (ID: 1)\n",
      "Token 230: '<eos>'              (ID: 1)\n",
      "Token 231: '<eos>'              (ID: 1)\n",
      "Token 232: '<eos>'              (ID: 1)\n",
      "Token 233: '<eos>'              (ID: 1)\n",
      "Token 234: '<eos>'              (ID: 1)\n",
      "Token 235: '<eos>'              (ID: 1)\n",
      "Token 236: '<eos>'              (ID: 1)\n",
      "Token 237: '<eos>'              (ID: 1)\n",
      "Token 238: '<eos>'              (ID: 1)\n",
      "Token 239: '<eos>'              (ID: 1)\n",
      "Token 240: '<eos>'              (ID: 1)\n",
      "Token 241: '<eos>'              (ID: 1)\n",
      "Token 242: '<eos>'              (ID: 1)\n",
      "Token 243: '<eos>'              (ID: 1)\n",
      "Token 244: '<eos>'              (ID: 1)\n",
      "Token 245: 'The'                (ID: 651)\n",
      "Token 246: ' sum'               (ID: 2707)\n",
      "Token 247: ' of'                (ID: 576)\n",
      "Token 248: ' '                  (ID: 235248)\n",
      "Token 249: '2'                  (ID: 235284) <-- (part of context or number)\n",
      "Token 250: '3'                  (ID: 235304) <-- (part of context or number)\n",
      "Token 251: ' and'               (ID: 578)\n",
      "Token 252: ' '                  (ID: 235248)\n",
      "Token 253: '4'                  (ID: 235310) <-- SECOND NUMBER digit '4'\n",
      "Token 254: '5'                  (ID: 235308) <-- SECOND NUMBER digit '5'\n",
      "Token 255: ' is'                (ID: 603)\n",
      "\n",
      "======================================================================\n",
      "KEY OBSERVATION: Numbers are tokenized DIGIT-BY-DIGIT\n",
      "'23' becomes two tokens, '45' becomes two tokens\n",
      "This means we CAN intervene on individual digits!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with Gemma using pipeline.load() for consistency with get_digit_token_position\n",
    "gemma_token_dict = gemma_pipeline.load({\"raw_input\": prompt}, add_special_tokens=False)\n",
    "gemma_tokens = gemma_token_dict[\"input_ids\"][0].tolist()\n",
    "\n",
    "print(\"Gemma 2 9B Tokenization:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total tokens: {len(gemma_tokens)}\\n\")\n",
    "\n",
    "for i, token_id in enumerate(gemma_tokens):\n",
    "    token_str = gemma_pipeline.tokenizer.decode([token_id])\n",
    "    marker = \"\"\n",
    "    if token_str.strip() in [\"2\", \"3\"]:\n",
    "        if i < len(gemma_tokens) // 2:\n",
    "            marker = f\" <-- FIRST NUMBER digit '{token_str.strip()}'\"\n",
    "        else:\n",
    "            marker = \" <-- (part of context or number)\"\n",
    "    elif token_str.strip() in [\"4\", \"5\"]:\n",
    "        marker = f\" <-- SECOND NUMBER digit '{token_str.strip()}'\"\n",
    "    print(f\"Token {i}: {repr(token_str):20s} (ID: {token_id}){marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY OBSERVATION: Numbers are tokenized DIGIT-BY-DIGIT\")\n",
    "print(\"'23' becomes two tokens, '45' becomes two tokens\")\n",
    "print(\"This means we CAN intervene on individual digits!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Side-by-Side Comparison\n",
    "\n",
    "Let's create a clear visual comparison of the two tokenization schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIDE-BY-SIDE COMPARISON\n",
      "======================================================================\n",
      "Prompt: 'The sum of 23 and 45 is'\n",
      "\n",
      "Llama 3.1 8B (whole-number tokens):\n",
      "----------------------------------------------------------------------\n",
      "  [0] '<|eot_id|>'\n",
      "  [1] '<|eot_id|>'\n",
      "  [2] '<|eot_id|>'\n",
      "  [3] '<|eot_id|>'\n",
      "  [4] '<|eot_id|>'\n",
      "  [5] '<|eot_id|>'\n",
      "  [6] '<|eot_id|>'\n",
      "  [7] '<|eot_id|>'\n",
      "  [8] '<|eot_id|>'\n",
      "  [9] '<|eot_id|>'\n",
      "  [10] '<|eot_id|>'\n",
      "  [11] '<|eot_id|>'\n",
      "  [12] '<|eot_id|>'\n",
      "  [13] '<|eot_id|>'\n",
      "  [14] '<|eot_id|>'\n",
      "  [15] '<|eot_id|>'\n",
      "  [16] '<|eot_id|>'\n",
      "  [17] '<|eot_id|>'\n",
      "  [18] '<|eot_id|>'\n",
      "  [19] '<|eot_id|>'\n",
      "  [20] '<|eot_id|>'\n",
      "  [21] '<|eot_id|>'\n",
      "  [22] '<|eot_id|>'\n",
      "  [23] '<|eot_id|>'\n",
      "  [24] '<|eot_id|>'\n",
      "  [25] '<|eot_id|>'\n",
      "  [26] '<|eot_id|>'\n",
      "  [27] '<|eot_id|>'\n",
      "  [28] '<|eot_id|>'\n",
      "  [29] '<|eot_id|>'\n",
      "  [30] '<|eot_id|>'\n",
      "  [31] '<|eot_id|>'\n",
      "  [32] '<|eot_id|>'\n",
      "  [33] '<|eot_id|>'\n",
      "  [34] '<|eot_id|>'\n",
      "  [35] '<|eot_id|>'\n",
      "  [36] '<|eot_id|>'\n",
      "  [37] '<|eot_id|>'\n",
      "  [38] '<|eot_id|>'\n",
      "  [39] '<|eot_id|>'\n",
      "  [40] '<|eot_id|>'\n",
      "  [41] '<|eot_id|>'\n",
      "  [42] '<|eot_id|>'\n",
      "  [43] '<|eot_id|>'\n",
      "  [44] '<|eot_id|>'\n",
      "  [45] '<|eot_id|>'\n",
      "  [46] '<|eot_id|>'\n",
      "  [47] '<|eot_id|>'\n",
      "  [48] '<|eot_id|>'\n",
      "  [49] '<|eot_id|>'\n",
      "  [50] '<|eot_id|>'\n",
      "  [51] '<|eot_id|>'\n",
      "  [52] '<|eot_id|>'\n",
      "  [53] '<|eot_id|>'\n",
      "  [54] '<|eot_id|>'\n",
      "  [55] '<|eot_id|>'\n",
      "  [56] '<|eot_id|>'\n",
      "  [57] '<|eot_id|>'\n",
      "  [58] '<|eot_id|>'\n",
      "  [59] '<|eot_id|>'\n",
      "  [60] '<|eot_id|>'\n",
      "  [61] '<|eot_id|>'\n",
      "  [62] '<|eot_id|>'\n",
      "  [63] '<|eot_id|>'\n",
      "  [64] '<|eot_id|>'\n",
      "  [65] '<|eot_id|>'\n",
      "  [66] '<|eot_id|>'\n",
      "  [67] '<|eot_id|>'\n",
      "  [68] '<|eot_id|>'\n",
      "  [69] '<|eot_id|>'\n",
      "  [70] '<|eot_id|>'\n",
      "  [71] '<|eot_id|>'\n",
      "  [72] '<|eot_id|>'\n",
      "  [73] '<|eot_id|>'\n",
      "  [74] '<|eot_id|>'\n",
      "  [75] '<|eot_id|>'\n",
      "  [76] '<|eot_id|>'\n",
      "  [77] '<|eot_id|>'\n",
      "  [78] '<|eot_id|>'\n",
      "  [79] '<|eot_id|>'\n",
      "  [80] '<|eot_id|>'\n",
      "  [81] '<|eot_id|>'\n",
      "  [82] '<|eot_id|>'\n",
      "  [83] '<|eot_id|>'\n",
      "  [84] '<|eot_id|>'\n",
      "  [85] '<|eot_id|>'\n",
      "  [86] '<|eot_id|>'\n",
      "  [87] '<|eot_id|>'\n",
      "  [88] '<|eot_id|>'\n",
      "  [89] '<|eot_id|>'\n",
      "  [90] '<|eot_id|>'\n",
      "  [91] '<|eot_id|>'\n",
      "  [92] '<|eot_id|>'\n",
      "  [93] '<|eot_id|>'\n",
      "  [94] '<|eot_id|>'\n",
      "  [95] '<|eot_id|>'\n",
      "  [96] '<|eot_id|>'\n",
      "  [97] '<|eot_id|>'\n",
      "  [98] '<|eot_id|>'\n",
      "  [99] '<|eot_id|>'\n",
      "  [100] '<|eot_id|>'\n",
      "  [101] '<|eot_id|>'\n",
      "  [102] '<|eot_id|>'\n",
      "  [103] '<|eot_id|>'\n",
      "  [104] '<|eot_id|>'\n",
      "  [105] '<|eot_id|>'\n",
      "  [106] '<|eot_id|>'\n",
      "  [107] '<|eot_id|>'\n",
      "  [108] '<|eot_id|>'\n",
      "  [109] '<|eot_id|>'\n",
      "  [110] '<|eot_id|>'\n",
      "  [111] '<|eot_id|>'\n",
      "  [112] '<|eot_id|>'\n",
      "  [113] '<|eot_id|>'\n",
      "  [114] '<|eot_id|>'\n",
      "  [115] '<|eot_id|>'\n",
      "  [116] '<|eot_id|>'\n",
      "  [117] '<|eot_id|>'\n",
      "  [118] '<|eot_id|>'\n",
      "  [119] '<|eot_id|>'\n",
      "  [120] '<|eot_id|>'\n",
      "  [121] '<|eot_id|>'\n",
      "  [122] '<|eot_id|>'\n",
      "  [123] '<|eot_id|>'\n",
      "  [124] '<|eot_id|>'\n",
      "  [125] '<|eot_id|>'\n",
      "  [126] '<|eot_id|>'\n",
      "  [127] '<|eot_id|>'\n",
      "  [128] '<|eot_id|>'\n",
      "  [129] '<|eot_id|>'\n",
      "  [130] '<|eot_id|>'\n",
      "  [131] '<|eot_id|>'\n",
      "  [132] '<|eot_id|>'\n",
      "  [133] '<|eot_id|>'\n",
      "  [134] '<|eot_id|>'\n",
      "  [135] '<|eot_id|>'\n",
      "  [136] '<|eot_id|>'\n",
      "  [137] '<|eot_id|>'\n",
      "  [138] '<|eot_id|>'\n",
      "  [139] '<|eot_id|>'\n",
      "  [140] '<|eot_id|>'\n",
      "  [141] '<|eot_id|>'\n",
      "  [142] '<|eot_id|>'\n",
      "  [143] '<|eot_id|>'\n",
      "  [144] '<|eot_id|>'\n",
      "  [145] '<|eot_id|>'\n",
      "  [146] '<|eot_id|>'\n",
      "  [147] '<|eot_id|>'\n",
      "  [148] '<|eot_id|>'\n",
      "  [149] '<|eot_id|>'\n",
      "  [150] '<|eot_id|>'\n",
      "  [151] '<|eot_id|>'\n",
      "  [152] '<|eot_id|>'\n",
      "  [153] '<|eot_id|>'\n",
      "  [154] '<|eot_id|>'\n",
      "  [155] '<|eot_id|>'\n",
      "  [156] '<|eot_id|>'\n",
      "  [157] '<|eot_id|>'\n",
      "  [158] '<|eot_id|>'\n",
      "  [159] '<|eot_id|>'\n",
      "  [160] '<|eot_id|>'\n",
      "  [161] '<|eot_id|>'\n",
      "  [162] '<|eot_id|>'\n",
      "  [163] '<|eot_id|>'\n",
      "  [164] '<|eot_id|>'\n",
      "  [165] '<|eot_id|>'\n",
      "  [166] '<|eot_id|>'\n",
      "  [167] '<|eot_id|>'\n",
      "  [168] '<|eot_id|>'\n",
      "  [169] '<|eot_id|>'\n",
      "  [170] '<|eot_id|>'\n",
      "  [171] '<|eot_id|>'\n",
      "  [172] '<|eot_id|>'\n",
      "  [173] '<|eot_id|>'\n",
      "  [174] '<|eot_id|>'\n",
      "  [175] '<|eot_id|>'\n",
      "  [176] '<|eot_id|>'\n",
      "  [177] '<|eot_id|>'\n",
      "  [178] '<|eot_id|>'\n",
      "  [179] '<|eot_id|>'\n",
      "  [180] '<|eot_id|>'\n",
      "  [181] '<|eot_id|>'\n",
      "  [182] '<|eot_id|>'\n",
      "  [183] '<|eot_id|>'\n",
      "  [184] '<|eot_id|>'\n",
      "  [185] '<|eot_id|>'\n",
      "  [186] '<|eot_id|>'\n",
      "  [187] '<|eot_id|>'\n",
      "  [188] '<|eot_id|>'\n",
      "  [189] '<|eot_id|>'\n",
      "  [190] '<|eot_id|>'\n",
      "  [191] '<|eot_id|>'\n",
      "  [192] '<|eot_id|>'\n",
      "  [193] '<|eot_id|>'\n",
      "  [194] '<|eot_id|>'\n",
      "  [195] '<|eot_id|>'\n",
      "  [196] '<|eot_id|>'\n",
      "  [197] '<|eot_id|>'\n",
      "  [198] '<|eot_id|>'\n",
      "  [199] '<|eot_id|>'\n",
      "  [200] '<|eot_id|>'\n",
      "  [201] '<|eot_id|>'\n",
      "  [202] '<|eot_id|>'\n",
      "  [203] '<|eot_id|>'\n",
      "  [204] '<|eot_id|>'\n",
      "  [205] '<|eot_id|>'\n",
      "  [206] '<|eot_id|>'\n",
      "  [207] '<|eot_id|>'\n",
      "  [208] '<|eot_id|>'\n",
      "  [209] '<|eot_id|>'\n",
      "  [210] '<|eot_id|>'\n",
      "  [211] '<|eot_id|>'\n",
      "  [212] '<|eot_id|>'\n",
      "  [213] '<|eot_id|>'\n",
      "  [214] '<|eot_id|>'\n",
      "  [215] '<|eot_id|>'\n",
      "  [216] '<|eot_id|>'\n",
      "  [217] '<|eot_id|>'\n",
      "  [218] '<|eot_id|>'\n",
      "  [219] '<|eot_id|>'\n",
      "  [220] '<|eot_id|>'\n",
      "  [221] '<|eot_id|>'\n",
      "  [222] '<|eot_id|>'\n",
      "  [223] '<|eot_id|>'\n",
      "  [224] '<|eot_id|>'\n",
      "  [225] '<|eot_id|>'\n",
      "  [226] '<|eot_id|>'\n",
      "  [227] '<|eot_id|>'\n",
      "  [228] '<|eot_id|>'\n",
      "  [229] '<|eot_id|>'\n",
      "  [230] '<|eot_id|>'\n",
      "  [231] '<|eot_id|>'\n",
      "  [232] '<|eot_id|>'\n",
      "  [233] '<|eot_id|>'\n",
      "  [234] '<|eot_id|>'\n",
      "  [235] '<|eot_id|>'\n",
      "  [236] '<|eot_id|>'\n",
      "  [237] '<|eot_id|>'\n",
      "  [238] '<|eot_id|>'\n",
      "  [239] '<|eot_id|>'\n",
      "  [240] '<|eot_id|>'\n",
      "  [241] '<|eot_id|>'\n",
      "  [242] '<|eot_id|>'\n",
      "  [243] '<|eot_id|>'\n",
      "  [244] '<|eot_id|>'\n",
      "  [245] '<|eot_id|>'\n",
      "  [246] '<|eot_id|>'\n",
      "  [247] 'The'\n",
      "  [248] ' sum'\n",
      "  [249] ' of'\n",
      "  [250] ' '\n",
      "  [251] '23'\n",
      "  [252] ' and'\n",
      "  [253] ' '\n",
      "  [254] '45'\n",
      "  [255] ' is'\n",
      "  Total: 256 tokens\n",
      "\n",
      "Gemma 2 9B (digit-by-digit tokens):\n",
      "----------------------------------------------------------------------\n",
      "  [0] '<eos>'\n",
      "  [1] '<eos>'\n",
      "  [2] '<eos>'\n",
      "  [3] '<eos>'\n",
      "  [4] '<eos>'\n",
      "  [5] '<eos>'\n",
      "  [6] '<eos>'\n",
      "  [7] '<eos>'\n",
      "  [8] '<eos>'\n",
      "  [9] '<eos>'\n",
      "  [10] '<eos>'\n",
      "  [11] '<eos>'\n",
      "  [12] '<eos>'\n",
      "  [13] '<eos>'\n",
      "  [14] '<eos>'\n",
      "  [15] '<eos>'\n",
      "  [16] '<eos>'\n",
      "  [17] '<eos>'\n",
      "  [18] '<eos>'\n",
      "  [19] '<eos>'\n",
      "  [20] '<eos>'\n",
      "  [21] '<eos>'\n",
      "  [22] '<eos>'\n",
      "  [23] '<eos>'\n",
      "  [24] '<eos>'\n",
      "  [25] '<eos>'\n",
      "  [26] '<eos>'\n",
      "  [27] '<eos>'\n",
      "  [28] '<eos>'\n",
      "  [29] '<eos>'\n",
      "  [30] '<eos>'\n",
      "  [31] '<eos>'\n",
      "  [32] '<eos>'\n",
      "  [33] '<eos>'\n",
      "  [34] '<eos>'\n",
      "  [35] '<eos>'\n",
      "  [36] '<eos>'\n",
      "  [37] '<eos>'\n",
      "  [38] '<eos>'\n",
      "  [39] '<eos>'\n",
      "  [40] '<eos>'\n",
      "  [41] '<eos>'\n",
      "  [42] '<eos>'\n",
      "  [43] '<eos>'\n",
      "  [44] '<eos>'\n",
      "  [45] '<eos>'\n",
      "  [46] '<eos>'\n",
      "  [47] '<eos>'\n",
      "  [48] '<eos>'\n",
      "  [49] '<eos>'\n",
      "  [50] '<eos>'\n",
      "  [51] '<eos>'\n",
      "  [52] '<eos>'\n",
      "  [53] '<eos>'\n",
      "  [54] '<eos>'\n",
      "  [55] '<eos>'\n",
      "  [56] '<eos>'\n",
      "  [57] '<eos>'\n",
      "  [58] '<eos>'\n",
      "  [59] '<eos>'\n",
      "  [60] '<eos>'\n",
      "  [61] '<eos>'\n",
      "  [62] '<eos>'\n",
      "  [63] '<eos>'\n",
      "  [64] '<eos>'\n",
      "  [65] '<eos>'\n",
      "  [66] '<eos>'\n",
      "  [67] '<eos>'\n",
      "  [68] '<eos>'\n",
      "  [69] '<eos>'\n",
      "  [70] '<eos>'\n",
      "  [71] '<eos>'\n",
      "  [72] '<eos>'\n",
      "  [73] '<eos>'\n",
      "  [74] '<eos>'\n",
      "  [75] '<eos>'\n",
      "  [76] '<eos>'\n",
      "  [77] '<eos>'\n",
      "  [78] '<eos>'\n",
      "  [79] '<eos>'\n",
      "  [80] '<eos>'\n",
      "  [81] '<eos>'\n",
      "  [82] '<eos>'\n",
      "  [83] '<eos>'\n",
      "  [84] '<eos>'\n",
      "  [85] '<eos>'\n",
      "  [86] '<eos>'\n",
      "  [87] '<eos>'\n",
      "  [88] '<eos>'\n",
      "  [89] '<eos>'\n",
      "  [90] '<eos>'\n",
      "  [91] '<eos>'\n",
      "  [92] '<eos>'\n",
      "  [93] '<eos>'\n",
      "  [94] '<eos>'\n",
      "  [95] '<eos>'\n",
      "  [96] '<eos>'\n",
      "  [97] '<eos>'\n",
      "  [98] '<eos>'\n",
      "  [99] '<eos>'\n",
      "  [100] '<eos>'\n",
      "  [101] '<eos>'\n",
      "  [102] '<eos>'\n",
      "  [103] '<eos>'\n",
      "  [104] '<eos>'\n",
      "  [105] '<eos>'\n",
      "  [106] '<eos>'\n",
      "  [107] '<eos>'\n",
      "  [108] '<eos>'\n",
      "  [109] '<eos>'\n",
      "  [110] '<eos>'\n",
      "  [111] '<eos>'\n",
      "  [112] '<eos>'\n",
      "  [113] '<eos>'\n",
      "  [114] '<eos>'\n",
      "  [115] '<eos>'\n",
      "  [116] '<eos>'\n",
      "  [117] '<eos>'\n",
      "  [118] '<eos>'\n",
      "  [119] '<eos>'\n",
      "  [120] '<eos>'\n",
      "  [121] '<eos>'\n",
      "  [122] '<eos>'\n",
      "  [123] '<eos>'\n",
      "  [124] '<eos>'\n",
      "  [125] '<eos>'\n",
      "  [126] '<eos>'\n",
      "  [127] '<eos>'\n",
      "  [128] '<eos>'\n",
      "  [129] '<eos>'\n",
      "  [130] '<eos>'\n",
      "  [131] '<eos>'\n",
      "  [132] '<eos>'\n",
      "  [133] '<eos>'\n",
      "  [134] '<eos>'\n",
      "  [135] '<eos>'\n",
      "  [136] '<eos>'\n",
      "  [137] '<eos>'\n",
      "  [138] '<eos>'\n",
      "  [139] '<eos>'\n",
      "  [140] '<eos>'\n",
      "  [141] '<eos>'\n",
      "  [142] '<eos>'\n",
      "  [143] '<eos>'\n",
      "  [144] '<eos>'\n",
      "  [145] '<eos>'\n",
      "  [146] '<eos>'\n",
      "  [147] '<eos>'\n",
      "  [148] '<eos>'\n",
      "  [149] '<eos>'\n",
      "  [150] '<eos>'\n",
      "  [151] '<eos>'\n",
      "  [152] '<eos>'\n",
      "  [153] '<eos>'\n",
      "  [154] '<eos>'\n",
      "  [155] '<eos>'\n",
      "  [156] '<eos>'\n",
      "  [157] '<eos>'\n",
      "  [158] '<eos>'\n",
      "  [159] '<eos>'\n",
      "  [160] '<eos>'\n",
      "  [161] '<eos>'\n",
      "  [162] '<eos>'\n",
      "  [163] '<eos>'\n",
      "  [164] '<eos>'\n",
      "  [165] '<eos>'\n",
      "  [166] '<eos>'\n",
      "  [167] '<eos>'\n",
      "  [168] '<eos>'\n",
      "  [169] '<eos>'\n",
      "  [170] '<eos>'\n",
      "  [171] '<eos>'\n",
      "  [172] '<eos>'\n",
      "  [173] '<eos>'\n",
      "  [174] '<eos>'\n",
      "  [175] '<eos>'\n",
      "  [176] '<eos>'\n",
      "  [177] '<eos>'\n",
      "  [178] '<eos>'\n",
      "  [179] '<eos>'\n",
      "  [180] '<eos>'\n",
      "  [181] '<eos>'\n",
      "  [182] '<eos>'\n",
      "  [183] '<eos>'\n",
      "  [184] '<eos>'\n",
      "  [185] '<eos>'\n",
      "  [186] '<eos>'\n",
      "  [187] '<eos>'\n",
      "  [188] '<eos>'\n",
      "  [189] '<eos>'\n",
      "  [190] '<eos>'\n",
      "  [191] '<eos>'\n",
      "  [192] '<eos>'\n",
      "  [193] '<eos>'\n",
      "  [194] '<eos>'\n",
      "  [195] '<eos>'\n",
      "  [196] '<eos>'\n",
      "  [197] '<eos>'\n",
      "  [198] '<eos>'\n",
      "  [199] '<eos>'\n",
      "  [200] '<eos>'\n",
      "  [201] '<eos>'\n",
      "  [202] '<eos>'\n",
      "  [203] '<eos>'\n",
      "  [204] '<eos>'\n",
      "  [205] '<eos>'\n",
      "  [206] '<eos>'\n",
      "  [207] '<eos>'\n",
      "  [208] '<eos>'\n",
      "  [209] '<eos>'\n",
      "  [210] '<eos>'\n",
      "  [211] '<eos>'\n",
      "  [212] '<eos>'\n",
      "  [213] '<eos>'\n",
      "  [214] '<eos>'\n",
      "  [215] '<eos>'\n",
      "  [216] '<eos>'\n",
      "  [217] '<eos>'\n",
      "  [218] '<eos>'\n",
      "  [219] '<eos>'\n",
      "  [220] '<eos>'\n",
      "  [221] '<eos>'\n",
      "  [222] '<eos>'\n",
      "  [223] '<eos>'\n",
      "  [224] '<eos>'\n",
      "  [225] '<eos>'\n",
      "  [226] '<eos>'\n",
      "  [227] '<eos>'\n",
      "  [228] '<eos>'\n",
      "  [229] '<eos>'\n",
      "  [230] '<eos>'\n",
      "  [231] '<eos>'\n",
      "  [232] '<eos>'\n",
      "  [233] '<eos>'\n",
      "  [234] '<eos>'\n",
      "  [235] '<eos>'\n",
      "  [236] '<eos>'\n",
      "  [237] '<eos>'\n",
      "  [238] '<eos>'\n",
      "  [239] '<eos>'\n",
      "  [240] '<eos>'\n",
      "  [241] '<eos>'\n",
      "  [242] '<eos>'\n",
      "  [243] '<eos>'\n",
      "  [244] '<eos>'\n",
      "  [245] 'The'\n",
      "  [246] ' sum'\n",
      "  [247] ' of'\n",
      "  [248] ' '\n",
      "  [249] '2'\n",
      "  [250] '3'\n",
      "  [251] ' and'\n",
      "  [252] ' '\n",
      "  [253] '4'\n",
      "  [254] '5'\n",
      "  [255] ' is'\n",
      "  Total: 256 tokens\n",
      "\n",
      "======================================================================\n",
      "Notice: Both models may have padding tokens (shown as empty strings or <eos>)\n",
      "The actual text tokens come after padding for Gemma (left-padding)\n",
      "The actual text tokens come first for Llama (right-padding)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"SIDE-BY-SIDE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "print(\"Llama 3.1 8B (whole-number tokens):\")\n",
    "print(\"-\" * 70)\n",
    "llama_decoded = [llama_pipeline.tokenizer.decode([tid]) for tid in llama_tokens]\n",
    "for i, tok in enumerate(llama_decoded):\n",
    "    print(f\"  [{i}] {repr(tok)}\")\n",
    "print(f\"  Total: {len(llama_tokens)} tokens\")\n",
    "\n",
    "print(\"\\nGemma 2 9B (digit-by-digit tokens):\")\n",
    "print(\"-\" * 70)\n",
    "gemma_decoded = [gemma_pipeline.tokenizer.decode([tid]) for tid in gemma_tokens]\n",
    "for i, tok in enumerate(gemma_decoded):\n",
    "    print(f\"  [{i}] {repr(tok)}\")\n",
    "print(f\"  Total: {len(gemma_tokens)} tokens\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Notice: Both models may have padding tokens (shown as empty strings or <eos>)\")\n",
    "print(\"The actual text tokens come after padding for Gemma (left-padding)\")\n",
    "print(\"The actual text tokens come first for Llama (right-padding)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Finding Specific Digit Positions\n",
    "\n",
    "Now let's use the `get_digit_token_position` function to locate specific digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding digit positions using get_digit_token_position()\n",
      "======================================================================\n",
      "\n",
      "digit_0_0 (tens place of first number - the '2' in '23'):\n",
      "  Llama: tokens [251]\n",
      "         decoded: ['23']\n",
      "  Gemma: tokens [249, 250]\n",
      "         decoded: ['2', '3']\n",
      "\n",
      "digit_0_1 (ones place of first number - the '3' in '23'):\n",
      "  Llama: tokens [251]\n",
      "         decoded: ['23']\n",
      "  Gemma: tokens [249, 250]\n",
      "         decoded: ['2', '3']\n",
      "\n",
      "======================================================================\n",
      "CRITICAL INSIGHT:\n",
      "  Llama: Both digit_0_0 and digit_0_1 return the SAME token (the whole number)\n",
      "  Gemma: digit_0_0 and digit_0_1 return DIFFERENT tokens (individual digits)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Finding digit positions using get_digit_token_position()\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find the tens digit of the first number (\"2\" in \"23\")\n",
    "llama_digit_0_0 = get_digit_token_position(input_sample, llama_pipeline, 0, 0)\n",
    "gemma_digit_0_0 = get_digit_token_position(input_sample, gemma_pipeline, 0, 0)\n",
    "\n",
    "print(\"\\ndigit_0_0 (tens place of first number - the '2' in '23'):\")\n",
    "print(f\"  Llama: tokens {llama_digit_0_0}\")\n",
    "llama_decoded_digit = [\n",
    "    llama_pipeline.tokenizer.decode([llama_tokens[i]]) for i in llama_digit_0_0\n",
    "]\n",
    "print(f\"         decoded: {llama_decoded_digit}\")\n",
    "print(f\"  Gemma: tokens {gemma_digit_0_0}\")\n",
    "gemma_decoded_digit = [\n",
    "    gemma_pipeline.tokenizer.decode([gemma_tokens[i]]) for i in gemma_digit_0_0\n",
    "]\n",
    "print(f\"         decoded: {gemma_decoded_digit}\")\n",
    "\n",
    "# Find the ones digit of the first number (\"3\" in \"23\")\n",
    "llama_digit_0_1 = get_digit_token_position(input_sample, llama_pipeline, 0, 1)\n",
    "gemma_digit_0_1 = get_digit_token_position(input_sample, gemma_pipeline, 0, 1)\n",
    "\n",
    "print(\"\\ndigit_0_1 (ones place of first number - the '3' in '23'):\")\n",
    "print(f\"  Llama: tokens {llama_digit_0_1}\")\n",
    "llama_decoded_digit = [\n",
    "    llama_pipeline.tokenizer.decode([llama_tokens[i]]) for i in llama_digit_0_1\n",
    "]\n",
    "print(f\"         decoded: {llama_decoded_digit}\")\n",
    "print(f\"  Gemma: tokens {gemma_digit_0_1}\")\n",
    "gemma_decoded_digit = [\n",
    "    gemma_pipeline.tokenizer.decode([gemma_tokens[i]]) for i in gemma_digit_0_1\n",
    "]\n",
    "print(f\"         decoded: {gemma_decoded_digit}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CRITICAL INSIGHT:\")\n",
    "print(\"  Llama: Both digit_0_0 and digit_0_1 return the SAME token (the whole number)\")\n",
    "print(\"  Gemma: digit_0_0 and digit_0_1 return DIFFERENT tokens (individual digits)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Implications for Interventions\n",
    "\n",
    "This tokenization difference has major implications for what we can test with interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERVENTION IMPLICATIONS\n",
      "======================================================================\n",
      "\n",
      "Scenario: We want to test carry propagation by intervening on the ones digit\n",
      "\n",
      "With Llama 3.1 8B:\n",
      "  - We request tokens for 'digit_0_1' (ones place)\n",
      "  - We get: [251]\n",
      "  - This token represents: 23\n",
      "  - ⚠️  We're intervening on the ENTIRE NUMBER '23', not just the ones digit!\n",
      "  - Interpretation: Tests how the model handles whole-number changes\n",
      "\n",
      "With Gemma 2 9B:\n",
      "  - We request tokens for 'digit_0_1' (ones place)\n",
      "  - We get: [249, 250]\n",
      "  - This token represents: 2\n",
      "  - ✓ We're intervening on JUST the ones digit!\n",
      "  - Interpretation: Tests how the model handles individual digit changes\n",
      "\n",
      "======================================================================\n",
      "Both are valid experiments, but they test different things!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"INTERVENTION IMPLICATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nScenario: We want to test carry propagation by intervening on the ones digit\")\n",
    "print(\"\\nWith Llama 3.1 8B:\")\n",
    "print(\"  - We request tokens for 'digit_0_1' (ones place)\")\n",
    "print(f\"  - We get: {llama_digit_0_1}\")\n",
    "print(\n",
    "    f\"  - This token represents: {llama_pipeline.tokenizer.decode([llama_tokens[llama_digit_0_1[0]]])}\"\n",
    ")\n",
    "print(\"  - ⚠️  We're intervening on the ENTIRE NUMBER '23', not just the ones digit!\")\n",
    "print(\"  - Interpretation: Tests how the model handles whole-number changes\")\n",
    "\n",
    "print(\"\\nWith Gemma 2 9B:\")\n",
    "print(\"  - We request tokens for 'digit_0_1' (ones place)\")\n",
    "print(f\"  - We get: {gemma_digit_0_1}\")\n",
    "print(\n",
    "    f\"  - This token represents: {gemma_pipeline.tokenizer.decode([gemma_tokens[gemma_digit_0_1[0]]])}\"\n",
    ")\n",
    "print(\"  - ✓ We're intervening on JUST the ones digit!\")\n",
    "print(\"  - Interpretation: Tests how the model handles individual digit changes\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Both are valid experiments, but they test different things!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: 3-Digit Numbers (123 + 456)\n",
    "\n",
    "Let's see how the models handle larger numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Digit Example: 'The sum of 123 and 456 is'\n",
      "\n",
      "Llama 3.1 8B (3-digit):\n",
      "  [0] '<|eot_id|>'\n",
      "  [1] '<|eot_id|>'\n",
      "  [2] '<|eot_id|>'\n",
      "  [3] '<|eot_id|>'\n",
      "  [4] '<|eot_id|>'\n",
      "  [5] '<|eot_id|>'\n",
      "  [6] '<|eot_id|>'\n",
      "  [7] '<|eot_id|>'\n",
      "  [8] '<|eot_id|>'\n",
      "  [9] '<|eot_id|>'\n",
      "  [10] '<|eot_id|>'\n",
      "  [11] '<|eot_id|>'\n",
      "  [12] '<|eot_id|>'\n",
      "  [13] '<|eot_id|>'\n",
      "  [14] '<|eot_id|>'\n",
      "  [15] '<|eot_id|>'\n",
      "  [16] '<|eot_id|>'\n",
      "  [17] '<|eot_id|>'\n",
      "  [18] '<|eot_id|>'\n",
      "  [19] '<|eot_id|>'\n",
      "  [20] '<|eot_id|>'\n",
      "  [21] '<|eot_id|>'\n",
      "  [22] '<|eot_id|>'\n",
      "  [23] '<|eot_id|>'\n",
      "  [24] '<|eot_id|>'\n",
      "  [25] '<|eot_id|>'\n",
      "  [26] '<|eot_id|>'\n",
      "  [27] '<|eot_id|>'\n",
      "  [28] '<|eot_id|>'\n",
      "  [29] '<|eot_id|>'\n",
      "  [30] '<|eot_id|>'\n",
      "  [31] '<|eot_id|>'\n",
      "  [32] '<|eot_id|>'\n",
      "  [33] '<|eot_id|>'\n",
      "  [34] '<|eot_id|>'\n",
      "  [35] '<|eot_id|>'\n",
      "  [36] '<|eot_id|>'\n",
      "  [37] '<|eot_id|>'\n",
      "  [38] '<|eot_id|>'\n",
      "  [39] '<|eot_id|>'\n",
      "  [40] '<|eot_id|>'\n",
      "  [41] '<|eot_id|>'\n",
      "  [42] '<|eot_id|>'\n",
      "  [43] '<|eot_id|>'\n",
      "  [44] '<|eot_id|>'\n",
      "  [45] '<|eot_id|>'\n",
      "  [46] '<|eot_id|>'\n",
      "  [47] '<|eot_id|>'\n",
      "  [48] '<|eot_id|>'\n",
      "  [49] '<|eot_id|>'\n",
      "  [50] '<|eot_id|>'\n",
      "  [51] '<|eot_id|>'\n",
      "  [52] '<|eot_id|>'\n",
      "  [53] '<|eot_id|>'\n",
      "  [54] '<|eot_id|>'\n",
      "  [55] '<|eot_id|>'\n",
      "  [56] '<|eot_id|>'\n",
      "  [57] '<|eot_id|>'\n",
      "  [58] '<|eot_id|>'\n",
      "  [59] '<|eot_id|>'\n",
      "  [60] '<|eot_id|>'\n",
      "  [61] '<|eot_id|>'\n",
      "  [62] '<|eot_id|>'\n",
      "  [63] '<|eot_id|>'\n",
      "  [64] '<|eot_id|>'\n",
      "  [65] '<|eot_id|>'\n",
      "  [66] '<|eot_id|>'\n",
      "  [67] '<|eot_id|>'\n",
      "  [68] '<|eot_id|>'\n",
      "  [69] '<|eot_id|>'\n",
      "  [70] '<|eot_id|>'\n",
      "  [71] '<|eot_id|>'\n",
      "  [72] '<|eot_id|>'\n",
      "  [73] '<|eot_id|>'\n",
      "  [74] '<|eot_id|>'\n",
      "  [75] '<|eot_id|>'\n",
      "  [76] '<|eot_id|>'\n",
      "  [77] '<|eot_id|>'\n",
      "  [78] '<|eot_id|>'\n",
      "  [79] '<|eot_id|>'\n",
      "  [80] '<|eot_id|>'\n",
      "  [81] '<|eot_id|>'\n",
      "  [82] '<|eot_id|>'\n",
      "  [83] '<|eot_id|>'\n",
      "  [84] '<|eot_id|>'\n",
      "  [85] '<|eot_id|>'\n",
      "  [86] '<|eot_id|>'\n",
      "  [87] '<|eot_id|>'\n",
      "  [88] '<|eot_id|>'\n",
      "  [89] '<|eot_id|>'\n",
      "  [90] '<|eot_id|>'\n",
      "  [91] '<|eot_id|>'\n",
      "  [92] '<|eot_id|>'\n",
      "  [93] '<|eot_id|>'\n",
      "  [94] '<|eot_id|>'\n",
      "  [95] '<|eot_id|>'\n",
      "  [96] '<|eot_id|>'\n",
      "  [97] '<|eot_id|>'\n",
      "  [98] '<|eot_id|>'\n",
      "  [99] '<|eot_id|>'\n",
      "  [100] '<|eot_id|>'\n",
      "  [101] '<|eot_id|>'\n",
      "  [102] '<|eot_id|>'\n",
      "  [103] '<|eot_id|>'\n",
      "  [104] '<|eot_id|>'\n",
      "  [105] '<|eot_id|>'\n",
      "  [106] '<|eot_id|>'\n",
      "  [107] '<|eot_id|>'\n",
      "  [108] '<|eot_id|>'\n",
      "  [109] '<|eot_id|>'\n",
      "  [110] '<|eot_id|>'\n",
      "  [111] '<|eot_id|>'\n",
      "  [112] '<|eot_id|>'\n",
      "  [113] '<|eot_id|>'\n",
      "  [114] '<|eot_id|>'\n",
      "  [115] '<|eot_id|>'\n",
      "  [116] '<|eot_id|>'\n",
      "  [117] '<|eot_id|>'\n",
      "  [118] '<|eot_id|>'\n",
      "  [119] '<|eot_id|>'\n",
      "  [120] '<|eot_id|>'\n",
      "  [121] '<|eot_id|>'\n",
      "  [122] '<|eot_id|>'\n",
      "  [123] '<|eot_id|>'\n",
      "  [124] '<|eot_id|>'\n",
      "  [125] '<|eot_id|>'\n",
      "  [126] '<|eot_id|>'\n",
      "  [127] '<|eot_id|>'\n",
      "  [128] '<|eot_id|>'\n",
      "  [129] '<|eot_id|>'\n",
      "  [130] '<|eot_id|>'\n",
      "  [131] '<|eot_id|>'\n",
      "  [132] '<|eot_id|>'\n",
      "  [133] '<|eot_id|>'\n",
      "  [134] '<|eot_id|>'\n",
      "  [135] '<|eot_id|>'\n",
      "  [136] '<|eot_id|>'\n",
      "  [137] '<|eot_id|>'\n",
      "  [138] '<|eot_id|>'\n",
      "  [139] '<|eot_id|>'\n",
      "  [140] '<|eot_id|>'\n",
      "  [141] '<|eot_id|>'\n",
      "  [142] '<|eot_id|>'\n",
      "  [143] '<|eot_id|>'\n",
      "  [144] '<|eot_id|>'\n",
      "  [145] '<|eot_id|>'\n",
      "  [146] '<|eot_id|>'\n",
      "  [147] '<|eot_id|>'\n",
      "  [148] '<|eot_id|>'\n",
      "  [149] '<|eot_id|>'\n",
      "  [150] '<|eot_id|>'\n",
      "  [151] '<|eot_id|>'\n",
      "  [152] '<|eot_id|>'\n",
      "  [153] '<|eot_id|>'\n",
      "  [154] '<|eot_id|>'\n",
      "  [155] '<|eot_id|>'\n",
      "  [156] '<|eot_id|>'\n",
      "  [157] '<|eot_id|>'\n",
      "  [158] '<|eot_id|>'\n",
      "  [159] '<|eot_id|>'\n",
      "  [160] '<|eot_id|>'\n",
      "  [161] '<|eot_id|>'\n",
      "  [162] '<|eot_id|>'\n",
      "  [163] '<|eot_id|>'\n",
      "  [164] '<|eot_id|>'\n",
      "  [165] '<|eot_id|>'\n",
      "  [166] '<|eot_id|>'\n",
      "  [167] '<|eot_id|>'\n",
      "  [168] '<|eot_id|>'\n",
      "  [169] '<|eot_id|>'\n",
      "  [170] '<|eot_id|>'\n",
      "  [171] '<|eot_id|>'\n",
      "  [172] '<|eot_id|>'\n",
      "  [173] '<|eot_id|>'\n",
      "  [174] '<|eot_id|>'\n",
      "  [175] '<|eot_id|>'\n",
      "  [176] '<|eot_id|>'\n",
      "  [177] '<|eot_id|>'\n",
      "  [178] '<|eot_id|>'\n",
      "  [179] '<|eot_id|>'\n",
      "  [180] '<|eot_id|>'\n",
      "  [181] '<|eot_id|>'\n",
      "  [182] '<|eot_id|>'\n",
      "  [183] '<|eot_id|>'\n",
      "  [184] '<|eot_id|>'\n",
      "  [185] '<|eot_id|>'\n",
      "  [186] '<|eot_id|>'\n",
      "  [187] '<|eot_id|>'\n",
      "  [188] '<|eot_id|>'\n",
      "  [189] '<|eot_id|>'\n",
      "  [190] '<|eot_id|>'\n",
      "  [191] '<|eot_id|>'\n",
      "  [192] '<|eot_id|>'\n",
      "  [193] '<|eot_id|>'\n",
      "  [194] '<|eot_id|>'\n",
      "  [195] '<|eot_id|>'\n",
      "  [196] '<|eot_id|>'\n",
      "  [197] '<|eot_id|>'\n",
      "  [198] '<|eot_id|>'\n",
      "  [199] '<|eot_id|>'\n",
      "  [200] '<|eot_id|>'\n",
      "  [201] '<|eot_id|>'\n",
      "  [202] '<|eot_id|>'\n",
      "  [203] '<|eot_id|>'\n",
      "  [204] '<|eot_id|>'\n",
      "  [205] '<|eot_id|>'\n",
      "  [206] '<|eot_id|>'\n",
      "  [207] '<|eot_id|>'\n",
      "  [208] '<|eot_id|>'\n",
      "  [209] '<|eot_id|>'\n",
      "  [210] '<|eot_id|>'\n",
      "  [211] '<|eot_id|>'\n",
      "  [212] '<|eot_id|>'\n",
      "  [213] '<|eot_id|>'\n",
      "  [214] '<|eot_id|>'\n",
      "  [215] '<|eot_id|>'\n",
      "  [216] '<|eot_id|>'\n",
      "  [217] '<|eot_id|>'\n",
      "  [218] '<|eot_id|>'\n",
      "  [219] '<|eot_id|>'\n",
      "  [220] '<|eot_id|>'\n",
      "  [221] '<|eot_id|>'\n",
      "  [222] '<|eot_id|>'\n",
      "  [223] '<|eot_id|>'\n",
      "  [224] '<|eot_id|>'\n",
      "  [225] '<|eot_id|>'\n",
      "  [226] '<|eot_id|>'\n",
      "  [227] '<|eot_id|>'\n",
      "  [228] '<|eot_id|>'\n",
      "  [229] '<|eot_id|>'\n",
      "  [230] '<|eot_id|>'\n",
      "  [231] '<|eot_id|>'\n",
      "  [232] '<|eot_id|>'\n",
      "  [233] '<|eot_id|>'\n",
      "  [234] '<|eot_id|>'\n",
      "  [235] '<|eot_id|>'\n",
      "  [236] '<|eot_id|>'\n",
      "  [237] '<|eot_id|>'\n",
      "  [238] '<|eot_id|>'\n",
      "  [239] '<|eot_id|>'\n",
      "  [240] '<|eot_id|>'\n",
      "  [241] '<|eot_id|>'\n",
      "  [242] '<|eot_id|>'\n",
      "  [243] '<|eot_id|>'\n",
      "  [244] '<|eot_id|>'\n",
      "  [245] '<|eot_id|>'\n",
      "  [246] '<|eot_id|>'\n",
      "  [247] 'The'\n",
      "  [248] ' sum'\n",
      "  [249] ' of'\n",
      "  [250] ' '\n",
      "  [251] '123' <-- NUMBER\n",
      "  [252] ' and'\n",
      "  [253] ' '\n",
      "  [254] '456' <-- NUMBER\n",
      "  [255] ' is'\n",
      "\n",
      "Gemma 2 9B (3-digit):\n",
      "  [0] '<eos>'\n",
      "  [1] '<eos>'\n",
      "  [2] '<eos>'\n",
      "  [3] '<eos>'\n",
      "  [4] '<eos>'\n",
      "  [5] '<eos>'\n",
      "  [6] '<eos>'\n",
      "  [7] '<eos>'\n",
      "  [8] '<eos>'\n",
      "  [9] '<eos>'\n",
      "  [10] '<eos>'\n",
      "  [11] '<eos>'\n",
      "  [12] '<eos>'\n",
      "  [13] '<eos>'\n",
      "  [14] '<eos>'\n",
      "  [15] '<eos>'\n",
      "  [16] '<eos>'\n",
      "  [17] '<eos>'\n",
      "  [18] '<eos>'\n",
      "  [19] '<eos>'\n",
      "  [20] '<eos>'\n",
      "  [21] '<eos>'\n",
      "  [22] '<eos>'\n",
      "  [23] '<eos>'\n",
      "  [24] '<eos>'\n",
      "  [25] '<eos>'\n",
      "  [26] '<eos>'\n",
      "  [27] '<eos>'\n",
      "  [28] '<eos>'\n",
      "  [29] '<eos>'\n",
      "  [30] '<eos>'\n",
      "  [31] '<eos>'\n",
      "  [32] '<eos>'\n",
      "  [33] '<eos>'\n",
      "  [34] '<eos>'\n",
      "  [35] '<eos>'\n",
      "  [36] '<eos>'\n",
      "  [37] '<eos>'\n",
      "  [38] '<eos>'\n",
      "  [39] '<eos>'\n",
      "  [40] '<eos>'\n",
      "  [41] '<eos>'\n",
      "  [42] '<eos>'\n",
      "  [43] '<eos>'\n",
      "  [44] '<eos>'\n",
      "  [45] '<eos>'\n",
      "  [46] '<eos>'\n",
      "  [47] '<eos>'\n",
      "  [48] '<eos>'\n",
      "  [49] '<eos>'\n",
      "  [50] '<eos>'\n",
      "  [51] '<eos>'\n",
      "  [52] '<eos>'\n",
      "  [53] '<eos>'\n",
      "  [54] '<eos>'\n",
      "  [55] '<eos>'\n",
      "  [56] '<eos>'\n",
      "  [57] '<eos>'\n",
      "  [58] '<eos>'\n",
      "  [59] '<eos>'\n",
      "  [60] '<eos>'\n",
      "  [61] '<eos>'\n",
      "  [62] '<eos>'\n",
      "  [63] '<eos>'\n",
      "  [64] '<eos>'\n",
      "  [65] '<eos>'\n",
      "  [66] '<eos>'\n",
      "  [67] '<eos>'\n",
      "  [68] '<eos>'\n",
      "  [69] '<eos>'\n",
      "  [70] '<eos>'\n",
      "  [71] '<eos>'\n",
      "  [72] '<eos>'\n",
      "  [73] '<eos>'\n",
      "  [74] '<eos>'\n",
      "  [75] '<eos>'\n",
      "  [76] '<eos>'\n",
      "  [77] '<eos>'\n",
      "  [78] '<eos>'\n",
      "  [79] '<eos>'\n",
      "  [80] '<eos>'\n",
      "  [81] '<eos>'\n",
      "  [82] '<eos>'\n",
      "  [83] '<eos>'\n",
      "  [84] '<eos>'\n",
      "  [85] '<eos>'\n",
      "  [86] '<eos>'\n",
      "  [87] '<eos>'\n",
      "  [88] '<eos>'\n",
      "  [89] '<eos>'\n",
      "  [90] '<eos>'\n",
      "  [91] '<eos>'\n",
      "  [92] '<eos>'\n",
      "  [93] '<eos>'\n",
      "  [94] '<eos>'\n",
      "  [95] '<eos>'\n",
      "  [96] '<eos>'\n",
      "  [97] '<eos>'\n",
      "  [98] '<eos>'\n",
      "  [99] '<eos>'\n",
      "  [100] '<eos>'\n",
      "  [101] '<eos>'\n",
      "  [102] '<eos>'\n",
      "  [103] '<eos>'\n",
      "  [104] '<eos>'\n",
      "  [105] '<eos>'\n",
      "  [106] '<eos>'\n",
      "  [107] '<eos>'\n",
      "  [108] '<eos>'\n",
      "  [109] '<eos>'\n",
      "  [110] '<eos>'\n",
      "  [111] '<eos>'\n",
      "  [112] '<eos>'\n",
      "  [113] '<eos>'\n",
      "  [114] '<eos>'\n",
      "  [115] '<eos>'\n",
      "  [116] '<eos>'\n",
      "  [117] '<eos>'\n",
      "  [118] '<eos>'\n",
      "  [119] '<eos>'\n",
      "  [120] '<eos>'\n",
      "  [121] '<eos>'\n",
      "  [122] '<eos>'\n",
      "  [123] '<eos>'\n",
      "  [124] '<eos>'\n",
      "  [125] '<eos>'\n",
      "  [126] '<eos>'\n",
      "  [127] '<eos>'\n",
      "  [128] '<eos>'\n",
      "  [129] '<eos>'\n",
      "  [130] '<eos>'\n",
      "  [131] '<eos>'\n",
      "  [132] '<eos>'\n",
      "  [133] '<eos>'\n",
      "  [134] '<eos>'\n",
      "  [135] '<eos>'\n",
      "  [136] '<eos>'\n",
      "  [137] '<eos>'\n",
      "  [138] '<eos>'\n",
      "  [139] '<eos>'\n",
      "  [140] '<eos>'\n",
      "  [141] '<eos>'\n",
      "  [142] '<eos>'\n",
      "  [143] '<eos>'\n",
      "  [144] '<eos>'\n",
      "  [145] '<eos>'\n",
      "  [146] '<eos>'\n",
      "  [147] '<eos>'\n",
      "  [148] '<eos>'\n",
      "  [149] '<eos>'\n",
      "  [150] '<eos>'\n",
      "  [151] '<eos>'\n",
      "  [152] '<eos>'\n",
      "  [153] '<eos>'\n",
      "  [154] '<eos>'\n",
      "  [155] '<eos>'\n",
      "  [156] '<eos>'\n",
      "  [157] '<eos>'\n",
      "  [158] '<eos>'\n",
      "  [159] '<eos>'\n",
      "  [160] '<eos>'\n",
      "  [161] '<eos>'\n",
      "  [162] '<eos>'\n",
      "  [163] '<eos>'\n",
      "  [164] '<eos>'\n",
      "  [165] '<eos>'\n",
      "  [166] '<eos>'\n",
      "  [167] '<eos>'\n",
      "  [168] '<eos>'\n",
      "  [169] '<eos>'\n",
      "  [170] '<eos>'\n",
      "  [171] '<eos>'\n",
      "  [172] '<eos>'\n",
      "  [173] '<eos>'\n",
      "  [174] '<eos>'\n",
      "  [175] '<eos>'\n",
      "  [176] '<eos>'\n",
      "  [177] '<eos>'\n",
      "  [178] '<eos>'\n",
      "  [179] '<eos>'\n",
      "  [180] '<eos>'\n",
      "  [181] '<eos>'\n",
      "  [182] '<eos>'\n",
      "  [183] '<eos>'\n",
      "  [184] '<eos>'\n",
      "  [185] '<eos>'\n",
      "  [186] '<eos>'\n",
      "  [187] '<eos>'\n",
      "  [188] '<eos>'\n",
      "  [189] '<eos>'\n",
      "  [190] '<eos>'\n",
      "  [191] '<eos>'\n",
      "  [192] '<eos>'\n",
      "  [193] '<eos>'\n",
      "  [194] '<eos>'\n",
      "  [195] '<eos>'\n",
      "  [196] '<eos>'\n",
      "  [197] '<eos>'\n",
      "  [198] '<eos>'\n",
      "  [199] '<eos>'\n",
      "  [200] '<eos>'\n",
      "  [201] '<eos>'\n",
      "  [202] '<eos>'\n",
      "  [203] '<eos>'\n",
      "  [204] '<eos>'\n",
      "  [205] '<eos>'\n",
      "  [206] '<eos>'\n",
      "  [207] '<eos>'\n",
      "  [208] '<eos>'\n",
      "  [209] '<eos>'\n",
      "  [210] '<eos>'\n",
      "  [211] '<eos>'\n",
      "  [212] '<eos>'\n",
      "  [213] '<eos>'\n",
      "  [214] '<eos>'\n",
      "  [215] '<eos>'\n",
      "  [216] '<eos>'\n",
      "  [217] '<eos>'\n",
      "  [218] '<eos>'\n",
      "  [219] '<eos>'\n",
      "  [220] '<eos>'\n",
      "  [221] '<eos>'\n",
      "  [222] '<eos>'\n",
      "  [223] '<eos>'\n",
      "  [224] '<eos>'\n",
      "  [225] '<eos>'\n",
      "  [226] '<eos>'\n",
      "  [227] '<eos>'\n",
      "  [228] '<eos>'\n",
      "  [229] '<eos>'\n",
      "  [230] '<eos>'\n",
      "  [231] '<eos>'\n",
      "  [232] '<eos>'\n",
      "  [233] '<eos>'\n",
      "  [234] '<eos>'\n",
      "  [235] '<eos>'\n",
      "  [236] '<eos>'\n",
      "  [237] '<eos>'\n",
      "  [238] '<eos>'\n",
      "  [239] '<eos>'\n",
      "  [240] '<eos>'\n",
      "  [241] '<eos>'\n",
      "  [242] '<eos>'\n",
      "  [243] 'The'\n",
      "  [244] ' sum'\n",
      "  [245] ' of'\n",
      "  [246] ' '\n",
      "  [247] '1' <-- DIGIT\n",
      "  [248] '2' <-- DIGIT\n",
      "  [249] '3' <-- DIGIT\n",
      "  [250] ' and'\n",
      "  [251] ' '\n",
      "  [252] '4' <-- DIGIT\n",
      "  [253] '5' <-- DIGIT\n",
      "  [254] '6' <-- DIGIT\n",
      "  [255] ' is'\n",
      "\n",
      "Observation:\n",
      "  Llama: STILL uses 1 token per number! ('123' and '456' are single tokens)\n",
      "  Gemma: Uses 3 tokens per number (digit-by-digit)\n"
     ]
    }
   ],
   "source": [
    "# Create 3-digit example\n",
    "config_3d = create_two_number_three_digit_config()\n",
    "model_3d = create_basic_addition_model(config_3d)\n",
    "\n",
    "input_sample_3d = {\n",
    "    \"digit_0_0\": 1,\n",
    "    \"digit_0_1\": 2,\n",
    "    \"digit_0_2\": 3,  # 123\n",
    "    \"digit_1_0\": 4,\n",
    "    \"digit_1_1\": 5,\n",
    "    \"digit_1_2\": 6,  # 456\n",
    "    \"num_addends\": 2,\n",
    "    \"num_digits\": 3,\n",
    "    \"template\": config_3d.templates[0],\n",
    "}\n",
    "\n",
    "output_3d = model_3d.new_trace(input_sample_3d)\n",
    "prompt_3d = output_3d['raw_input']\n",
    "input_sample_3d['raw_input'] = prompt_3d  # Add raw_input to sample for later use\n",
    "\n",
    "print(f\"3-Digit Example: '{prompt_3d}'\\n\")\n",
    "\n",
    "# Tokenize with both models using pipeline.load()\n",
    "llama_token_dict_3d = llama_pipeline.load(\n",
    "    {\"raw_input\": prompt_3d}, add_special_tokens=False\n",
    ")\n",
    "llama_tokens_3d = llama_token_dict_3d[\"input_ids\"][0].tolist()\n",
    "\n",
    "gemma_token_dict_3d = gemma_pipeline.load(\n",
    "    {\"raw_input\": prompt_3d}, add_special_tokens=False\n",
    ")\n",
    "gemma_tokens_3d = gemma_token_dict_3d[\"input_ids\"][0].tolist()\n",
    "\n",
    "print(\"Llama 3.1 8B (3-digit):\")\n",
    "for i, tok_id in enumerate(llama_tokens_3d):\n",
    "    tok = llama_pipeline.tokenizer.decode([tok_id])\n",
    "    marker = \" <-- NUMBER\" if any(d in tok for d in [\"123\", \"456\"]) else \"\"\n",
    "    print(f\"  [{i}] {repr(tok)}{marker}\")\n",
    "\n",
    "print(\"\\nGemma 2 9B (3-digit):\")\n",
    "for i, tok_id in enumerate(gemma_tokens_3d):\n",
    "    tok = gemma_pipeline.tokenizer.decode([tok_id])\n",
    "    marker = \" <-- DIGIT\" if tok.strip() in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"] else \"\"\n",
    "    print(f\"  [{i}] {repr(tok)}{marker}\")\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  Llama: STILL uses 1 token per number! ('123' and '456' are single tokens)\")\n",
    "print(\"  Gemma: Uses 3 tokens per number (digit-by-digit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: 4-Digit Numbers (1234 + 5678)\n",
    "\n",
    "Now let's push it further with 4-digit numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Digit Example: 'The sum of 1234 and 5678 is'\n",
      "\n",
      "Llama 3.1 8B (4-digit):\n",
      "  [0] '<|eot_id|>'\n",
      "  [1] '<|eot_id|>'\n",
      "  [2] '<|eot_id|>'\n",
      "  [3] '<|eot_id|>'\n",
      "  [4] '<|eot_id|>'\n",
      "  [5] '<|eot_id|>'\n",
      "  [6] '<|eot_id|>'\n",
      "  [7] '<|eot_id|>'\n",
      "  [8] '<|eot_id|>'\n",
      "  [9] '<|eot_id|>'\n",
      "  [10] '<|eot_id|>'\n",
      "  [11] '<|eot_id|>'\n",
      "  [12] '<|eot_id|>'\n",
      "  [13] '<|eot_id|>'\n",
      "  [14] '<|eot_id|>'\n",
      "  [15] '<|eot_id|>'\n",
      "  [16] '<|eot_id|>'\n",
      "  [17] '<|eot_id|>'\n",
      "  [18] '<|eot_id|>'\n",
      "  [19] '<|eot_id|>'\n",
      "  [20] '<|eot_id|>'\n",
      "  [21] '<|eot_id|>'\n",
      "  [22] '<|eot_id|>'\n",
      "  [23] '<|eot_id|>'\n",
      "  [24] '<|eot_id|>'\n",
      "  [25] '<|eot_id|>'\n",
      "  [26] '<|eot_id|>'\n",
      "  [27] '<|eot_id|>'\n",
      "  [28] '<|eot_id|>'\n",
      "  [29] '<|eot_id|>'\n",
      "  [30] '<|eot_id|>'\n",
      "  [31] '<|eot_id|>'\n",
      "  [32] '<|eot_id|>'\n",
      "  [33] '<|eot_id|>'\n",
      "  [34] '<|eot_id|>'\n",
      "  [35] '<|eot_id|>'\n",
      "  [36] '<|eot_id|>'\n",
      "  [37] '<|eot_id|>'\n",
      "  [38] '<|eot_id|>'\n",
      "  [39] '<|eot_id|>'\n",
      "  [40] '<|eot_id|>'\n",
      "  [41] '<|eot_id|>'\n",
      "  [42] '<|eot_id|>'\n",
      "  [43] '<|eot_id|>'\n",
      "  [44] '<|eot_id|>'\n",
      "  [45] '<|eot_id|>'\n",
      "  [46] '<|eot_id|>'\n",
      "  [47] '<|eot_id|>'\n",
      "  [48] '<|eot_id|>'\n",
      "  [49] '<|eot_id|>'\n",
      "  [50] '<|eot_id|>'\n",
      "  [51] '<|eot_id|>'\n",
      "  [52] '<|eot_id|>'\n",
      "  [53] '<|eot_id|>'\n",
      "  [54] '<|eot_id|>'\n",
      "  [55] '<|eot_id|>'\n",
      "  [56] '<|eot_id|>'\n",
      "  [57] '<|eot_id|>'\n",
      "  [58] '<|eot_id|>'\n",
      "  [59] '<|eot_id|>'\n",
      "  [60] '<|eot_id|>'\n",
      "  [61] '<|eot_id|>'\n",
      "  [62] '<|eot_id|>'\n",
      "  [63] '<|eot_id|>'\n",
      "  [64] '<|eot_id|>'\n",
      "  [65] '<|eot_id|>'\n",
      "  [66] '<|eot_id|>'\n",
      "  [67] '<|eot_id|>'\n",
      "  [68] '<|eot_id|>'\n",
      "  [69] '<|eot_id|>'\n",
      "  [70] '<|eot_id|>'\n",
      "  [71] '<|eot_id|>'\n",
      "  [72] '<|eot_id|>'\n",
      "  [73] '<|eot_id|>'\n",
      "  [74] '<|eot_id|>'\n",
      "  [75] '<|eot_id|>'\n",
      "  [76] '<|eot_id|>'\n",
      "  [77] '<|eot_id|>'\n",
      "  [78] '<|eot_id|>'\n",
      "  [79] '<|eot_id|>'\n",
      "  [80] '<|eot_id|>'\n",
      "  [81] '<|eot_id|>'\n",
      "  [82] '<|eot_id|>'\n",
      "  [83] '<|eot_id|>'\n",
      "  [84] '<|eot_id|>'\n",
      "  [85] '<|eot_id|>'\n",
      "  [86] '<|eot_id|>'\n",
      "  [87] '<|eot_id|>'\n",
      "  [88] '<|eot_id|>'\n",
      "  [89] '<|eot_id|>'\n",
      "  [90] '<|eot_id|>'\n",
      "  [91] '<|eot_id|>'\n",
      "  [92] '<|eot_id|>'\n",
      "  [93] '<|eot_id|>'\n",
      "  [94] '<|eot_id|>'\n",
      "  [95] '<|eot_id|>'\n",
      "  [96] '<|eot_id|>'\n",
      "  [97] '<|eot_id|>'\n",
      "  [98] '<|eot_id|>'\n",
      "  [99] '<|eot_id|>'\n",
      "  [100] '<|eot_id|>'\n",
      "  [101] '<|eot_id|>'\n",
      "  [102] '<|eot_id|>'\n",
      "  [103] '<|eot_id|>'\n",
      "  [104] '<|eot_id|>'\n",
      "  [105] '<|eot_id|>'\n",
      "  [106] '<|eot_id|>'\n",
      "  [107] '<|eot_id|>'\n",
      "  [108] '<|eot_id|>'\n",
      "  [109] '<|eot_id|>'\n",
      "  [110] '<|eot_id|>'\n",
      "  [111] '<|eot_id|>'\n",
      "  [112] '<|eot_id|>'\n",
      "  [113] '<|eot_id|>'\n",
      "  [114] '<|eot_id|>'\n",
      "  [115] '<|eot_id|>'\n",
      "  [116] '<|eot_id|>'\n",
      "  [117] '<|eot_id|>'\n",
      "  [118] '<|eot_id|>'\n",
      "  [119] '<|eot_id|>'\n",
      "  [120] '<|eot_id|>'\n",
      "  [121] '<|eot_id|>'\n",
      "  [122] '<|eot_id|>'\n",
      "  [123] '<|eot_id|>'\n",
      "  [124] '<|eot_id|>'\n",
      "  [125] '<|eot_id|>'\n",
      "  [126] '<|eot_id|>'\n",
      "  [127] '<|eot_id|>'\n",
      "  [128] '<|eot_id|>'\n",
      "  [129] '<|eot_id|>'\n",
      "  [130] '<|eot_id|>'\n",
      "  [131] '<|eot_id|>'\n",
      "  [132] '<|eot_id|>'\n",
      "  [133] '<|eot_id|>'\n",
      "  [134] '<|eot_id|>'\n",
      "  [135] '<|eot_id|>'\n",
      "  [136] '<|eot_id|>'\n",
      "  [137] '<|eot_id|>'\n",
      "  [138] '<|eot_id|>'\n",
      "  [139] '<|eot_id|>'\n",
      "  [140] '<|eot_id|>'\n",
      "  [141] '<|eot_id|>'\n",
      "  [142] '<|eot_id|>'\n",
      "  [143] '<|eot_id|>'\n",
      "  [144] '<|eot_id|>'\n",
      "  [145] '<|eot_id|>'\n",
      "  [146] '<|eot_id|>'\n",
      "  [147] '<|eot_id|>'\n",
      "  [148] '<|eot_id|>'\n",
      "  [149] '<|eot_id|>'\n",
      "  [150] '<|eot_id|>'\n",
      "  [151] '<|eot_id|>'\n",
      "  [152] '<|eot_id|>'\n",
      "  [153] '<|eot_id|>'\n",
      "  [154] '<|eot_id|>'\n",
      "  [155] '<|eot_id|>'\n",
      "  [156] '<|eot_id|>'\n",
      "  [157] '<|eot_id|>'\n",
      "  [158] '<|eot_id|>'\n",
      "  [159] '<|eot_id|>'\n",
      "  [160] '<|eot_id|>'\n",
      "  [161] '<|eot_id|>'\n",
      "  [162] '<|eot_id|>'\n",
      "  [163] '<|eot_id|>'\n",
      "  [164] '<|eot_id|>'\n",
      "  [165] '<|eot_id|>'\n",
      "  [166] '<|eot_id|>'\n",
      "  [167] '<|eot_id|>'\n",
      "  [168] '<|eot_id|>'\n",
      "  [169] '<|eot_id|>'\n",
      "  [170] '<|eot_id|>'\n",
      "  [171] '<|eot_id|>'\n",
      "  [172] '<|eot_id|>'\n",
      "  [173] '<|eot_id|>'\n",
      "  [174] '<|eot_id|>'\n",
      "  [175] '<|eot_id|>'\n",
      "  [176] '<|eot_id|>'\n",
      "  [177] '<|eot_id|>'\n",
      "  [178] '<|eot_id|>'\n",
      "  [179] '<|eot_id|>'\n",
      "  [180] '<|eot_id|>'\n",
      "  [181] '<|eot_id|>'\n",
      "  [182] '<|eot_id|>'\n",
      "  [183] '<|eot_id|>'\n",
      "  [184] '<|eot_id|>'\n",
      "  [185] '<|eot_id|>'\n",
      "  [186] '<|eot_id|>'\n",
      "  [187] '<|eot_id|>'\n",
      "  [188] '<|eot_id|>'\n",
      "  [189] '<|eot_id|>'\n",
      "  [190] '<|eot_id|>'\n",
      "  [191] '<|eot_id|>'\n",
      "  [192] '<|eot_id|>'\n",
      "  [193] '<|eot_id|>'\n",
      "  [194] '<|eot_id|>'\n",
      "  [195] '<|eot_id|>'\n",
      "  [196] '<|eot_id|>'\n",
      "  [197] '<|eot_id|>'\n",
      "  [198] '<|eot_id|>'\n",
      "  [199] '<|eot_id|>'\n",
      "  [200] '<|eot_id|>'\n",
      "  [201] '<|eot_id|>'\n",
      "  [202] '<|eot_id|>'\n",
      "  [203] '<|eot_id|>'\n",
      "  [204] '<|eot_id|>'\n",
      "  [205] '<|eot_id|>'\n",
      "  [206] '<|eot_id|>'\n",
      "  [207] '<|eot_id|>'\n",
      "  [208] '<|eot_id|>'\n",
      "  [209] '<|eot_id|>'\n",
      "  [210] '<|eot_id|>'\n",
      "  [211] '<|eot_id|>'\n",
      "  [212] '<|eot_id|>'\n",
      "  [213] '<|eot_id|>'\n",
      "  [214] '<|eot_id|>'\n",
      "  [215] '<|eot_id|>'\n",
      "  [216] '<|eot_id|>'\n",
      "  [217] '<|eot_id|>'\n",
      "  [218] '<|eot_id|>'\n",
      "  [219] '<|eot_id|>'\n",
      "  [220] '<|eot_id|>'\n",
      "  [221] '<|eot_id|>'\n",
      "  [222] '<|eot_id|>'\n",
      "  [223] '<|eot_id|>'\n",
      "  [224] '<|eot_id|>'\n",
      "  [225] '<|eot_id|>'\n",
      "  [226] '<|eot_id|>'\n",
      "  [227] '<|eot_id|>'\n",
      "  [228] '<|eot_id|>'\n",
      "  [229] '<|eot_id|>'\n",
      "  [230] '<|eot_id|>'\n",
      "  [231] '<|eot_id|>'\n",
      "  [232] '<|eot_id|>'\n",
      "  [233] '<|eot_id|>'\n",
      "  [234] '<|eot_id|>'\n",
      "  [235] '<|eot_id|>'\n",
      "  [236] '<|eot_id|>'\n",
      "  [237] '<|eot_id|>'\n",
      "  [238] '<|eot_id|>'\n",
      "  [239] '<|eot_id|>'\n",
      "  [240] '<|eot_id|>'\n",
      "  [241] '<|eot_id|>'\n",
      "  [242] '<|eot_id|>'\n",
      "  [243] '<|eot_id|>'\n",
      "  [244] '<|eot_id|>'\n",
      "  [245] 'The'\n",
      "  [246] ' sum'\n",
      "  [247] ' of'\n",
      "  [248] ' '\n",
      "  [249] '123' <-- NUMBER TOKEN\n",
      "  [250] '4'\n",
      "  [251] ' and'\n",
      "  [252] ' '\n",
      "  [253] '567' <-- NUMBER TOKEN\n",
      "  [254] '8'\n",
      "  [255] ' is'\n",
      "\n",
      "Gemma 2 9B (4-digit):\n",
      "  [0] '<eos>'\n",
      "  [1] '<eos>'\n",
      "  [2] '<eos>'\n",
      "  [3] '<eos>'\n",
      "  [4] '<eos>'\n",
      "  [5] '<eos>'\n",
      "  [6] '<eos>'\n",
      "  [7] '<eos>'\n",
      "  [8] '<eos>'\n",
      "  [9] '<eos>'\n",
      "  [10] '<eos>'\n",
      "  [11] '<eos>'\n",
      "  [12] '<eos>'\n",
      "  [13] '<eos>'\n",
      "  [14] '<eos>'\n",
      "  [15] '<eos>'\n",
      "  [16] '<eos>'\n",
      "  [17] '<eos>'\n",
      "  [18] '<eos>'\n",
      "  [19] '<eos>'\n",
      "  [20] '<eos>'\n",
      "  [21] '<eos>'\n",
      "  [22] '<eos>'\n",
      "  [23] '<eos>'\n",
      "  [24] '<eos>'\n",
      "  [25] '<eos>'\n",
      "  [26] '<eos>'\n",
      "  [27] '<eos>'\n",
      "  [28] '<eos>'\n",
      "  [29] '<eos>'\n",
      "  [30] '<eos>'\n",
      "  [31] '<eos>'\n",
      "  [32] '<eos>'\n",
      "  [33] '<eos>'\n",
      "  [34] '<eos>'\n",
      "  [35] '<eos>'\n",
      "  [36] '<eos>'\n",
      "  [37] '<eos>'\n",
      "  [38] '<eos>'\n",
      "  [39] '<eos>'\n",
      "  [40] '<eos>'\n",
      "  [41] '<eos>'\n",
      "  [42] '<eos>'\n",
      "  [43] '<eos>'\n",
      "  [44] '<eos>'\n",
      "  [45] '<eos>'\n",
      "  [46] '<eos>'\n",
      "  [47] '<eos>'\n",
      "  [48] '<eos>'\n",
      "  [49] '<eos>'\n",
      "  [50] '<eos>'\n",
      "  [51] '<eos>'\n",
      "  [52] '<eos>'\n",
      "  [53] '<eos>'\n",
      "  [54] '<eos>'\n",
      "  [55] '<eos>'\n",
      "  [56] '<eos>'\n",
      "  [57] '<eos>'\n",
      "  [58] '<eos>'\n",
      "  [59] '<eos>'\n",
      "  [60] '<eos>'\n",
      "  [61] '<eos>'\n",
      "  [62] '<eos>'\n",
      "  [63] '<eos>'\n",
      "  [64] '<eos>'\n",
      "  [65] '<eos>'\n",
      "  [66] '<eos>'\n",
      "  [67] '<eos>'\n",
      "  [68] '<eos>'\n",
      "  [69] '<eos>'\n",
      "  [70] '<eos>'\n",
      "  [71] '<eos>'\n",
      "  [72] '<eos>'\n",
      "  [73] '<eos>'\n",
      "  [74] '<eos>'\n",
      "  [75] '<eos>'\n",
      "  [76] '<eos>'\n",
      "  [77] '<eos>'\n",
      "  [78] '<eos>'\n",
      "  [79] '<eos>'\n",
      "  [80] '<eos>'\n",
      "  [81] '<eos>'\n",
      "  [82] '<eos>'\n",
      "  [83] '<eos>'\n",
      "  [84] '<eos>'\n",
      "  [85] '<eos>'\n",
      "  [86] '<eos>'\n",
      "  [87] '<eos>'\n",
      "  [88] '<eos>'\n",
      "  [89] '<eos>'\n",
      "  [90] '<eos>'\n",
      "  [91] '<eos>'\n",
      "  [92] '<eos>'\n",
      "  [93] '<eos>'\n",
      "  [94] '<eos>'\n",
      "  [95] '<eos>'\n",
      "  [96] '<eos>'\n",
      "  [97] '<eos>'\n",
      "  [98] '<eos>'\n",
      "  [99] '<eos>'\n",
      "  [100] '<eos>'\n",
      "  [101] '<eos>'\n",
      "  [102] '<eos>'\n",
      "  [103] '<eos>'\n",
      "  [104] '<eos>'\n",
      "  [105] '<eos>'\n",
      "  [106] '<eos>'\n",
      "  [107] '<eos>'\n",
      "  [108] '<eos>'\n",
      "  [109] '<eos>'\n",
      "  [110] '<eos>'\n",
      "  [111] '<eos>'\n",
      "  [112] '<eos>'\n",
      "  [113] '<eos>'\n",
      "  [114] '<eos>'\n",
      "  [115] '<eos>'\n",
      "  [116] '<eos>'\n",
      "  [117] '<eos>'\n",
      "  [118] '<eos>'\n",
      "  [119] '<eos>'\n",
      "  [120] '<eos>'\n",
      "  [121] '<eos>'\n",
      "  [122] '<eos>'\n",
      "  [123] '<eos>'\n",
      "  [124] '<eos>'\n",
      "  [125] '<eos>'\n",
      "  [126] '<eos>'\n",
      "  [127] '<eos>'\n",
      "  [128] '<eos>'\n",
      "  [129] '<eos>'\n",
      "  [130] '<eos>'\n",
      "  [131] '<eos>'\n",
      "  [132] '<eos>'\n",
      "  [133] '<eos>'\n",
      "  [134] '<eos>'\n",
      "  [135] '<eos>'\n",
      "  [136] '<eos>'\n",
      "  [137] '<eos>'\n",
      "  [138] '<eos>'\n",
      "  [139] '<eos>'\n",
      "  [140] '<eos>'\n",
      "  [141] '<eos>'\n",
      "  [142] '<eos>'\n",
      "  [143] '<eos>'\n",
      "  [144] '<eos>'\n",
      "  [145] '<eos>'\n",
      "  [146] '<eos>'\n",
      "  [147] '<eos>'\n",
      "  [148] '<eos>'\n",
      "  [149] '<eos>'\n",
      "  [150] '<eos>'\n",
      "  [151] '<eos>'\n",
      "  [152] '<eos>'\n",
      "  [153] '<eos>'\n",
      "  [154] '<eos>'\n",
      "  [155] '<eos>'\n",
      "  [156] '<eos>'\n",
      "  [157] '<eos>'\n",
      "  [158] '<eos>'\n",
      "  [159] '<eos>'\n",
      "  [160] '<eos>'\n",
      "  [161] '<eos>'\n",
      "  [162] '<eos>'\n",
      "  [163] '<eos>'\n",
      "  [164] '<eos>'\n",
      "  [165] '<eos>'\n",
      "  [166] '<eos>'\n",
      "  [167] '<eos>'\n",
      "  [168] '<eos>'\n",
      "  [169] '<eos>'\n",
      "  [170] '<eos>'\n",
      "  [171] '<eos>'\n",
      "  [172] '<eos>'\n",
      "  [173] '<eos>'\n",
      "  [174] '<eos>'\n",
      "  [175] '<eos>'\n",
      "  [176] '<eos>'\n",
      "  [177] '<eos>'\n",
      "  [178] '<eos>'\n",
      "  [179] '<eos>'\n",
      "  [180] '<eos>'\n",
      "  [181] '<eos>'\n",
      "  [182] '<eos>'\n",
      "  [183] '<eos>'\n",
      "  [184] '<eos>'\n",
      "  [185] '<eos>'\n",
      "  [186] '<eos>'\n",
      "  [187] '<eos>'\n",
      "  [188] '<eos>'\n",
      "  [189] '<eos>'\n",
      "  [190] '<eos>'\n",
      "  [191] '<eos>'\n",
      "  [192] '<eos>'\n",
      "  [193] '<eos>'\n",
      "  [194] '<eos>'\n",
      "  [195] '<eos>'\n",
      "  [196] '<eos>'\n",
      "  [197] '<eos>'\n",
      "  [198] '<eos>'\n",
      "  [199] '<eos>'\n",
      "  [200] '<eos>'\n",
      "  [201] '<eos>'\n",
      "  [202] '<eos>'\n",
      "  [203] '<eos>'\n",
      "  [204] '<eos>'\n",
      "  [205] '<eos>'\n",
      "  [206] '<eos>'\n",
      "  [207] '<eos>'\n",
      "  [208] '<eos>'\n",
      "  [209] '<eos>'\n",
      "  [210] '<eos>'\n",
      "  [211] '<eos>'\n",
      "  [212] '<eos>'\n",
      "  [213] '<eos>'\n",
      "  [214] '<eos>'\n",
      "  [215] '<eos>'\n",
      "  [216] '<eos>'\n",
      "  [217] '<eos>'\n",
      "  [218] '<eos>'\n",
      "  [219] '<eos>'\n",
      "  [220] '<eos>'\n",
      "  [221] '<eos>'\n",
      "  [222] '<eos>'\n",
      "  [223] '<eos>'\n",
      "  [224] '<eos>'\n",
      "  [225] '<eos>'\n",
      "  [226] '<eos>'\n",
      "  [227] '<eos>'\n",
      "  [228] '<eos>'\n",
      "  [229] '<eos>'\n",
      "  [230] '<eos>'\n",
      "  [231] '<eos>'\n",
      "  [232] '<eos>'\n",
      "  [233] '<eos>'\n",
      "  [234] '<eos>'\n",
      "  [235] '<eos>'\n",
      "  [236] '<eos>'\n",
      "  [237] '<eos>'\n",
      "  [238] '<eos>'\n",
      "  [239] '<eos>'\n",
      "  [240] '<eos>'\n",
      "  [241] 'The'\n",
      "  [242] ' sum'\n",
      "  [243] ' of'\n",
      "  [244] ' '\n",
      "  [245] '1' <-- DIGIT\n",
      "  [246] '2' <-- DIGIT\n",
      "  [247] '3' <-- DIGIT\n",
      "  [248] '4' <-- DIGIT\n",
      "  [249] ' and'\n",
      "  [250] ' '\n",
      "  [251] '5' <-- DIGIT\n",
      "  [252] '6' <-- DIGIT\n",
      "  [253] '7' <-- DIGIT\n",
      "  [254] '8' <-- DIGIT\n",
      "  [255] ' is'\n",
      "\n",
      "Observation:\n",
      "  Llama: NOW splits into 2 tokens per number (e.g., '12' and '34' for '1234')\n",
      "  Gemma: Still uses 4 tokens per number (digit-by-digit as always)\n",
      "\n",
      "Llama's vocabulary has limits - very large numbers get split!\n"
     ]
    }
   ],
   "source": [
    "# Create 4-digit example\n",
    "config_4d = create_general_config(2, 4)\n",
    "model_4d = create_basic_addition_model(config_4d)\n",
    "\n",
    "input_sample_4d = {\n",
    "    \"digit_0_0\": 1,\n",
    "    \"digit_0_1\": 2,\n",
    "    \"digit_0_2\": 3,\n",
    "    \"digit_0_3\": 4,  # 1234\n",
    "    \"digit_1_0\": 5,\n",
    "    \"digit_1_1\": 6,\n",
    "    \"digit_1_2\": 7,\n",
    "    \"digit_1_3\": 8,  # 5678\n",
    "    \"num_addends\": 2,\n",
    "    \"num_digits\": 4,\n",
    "    \"template\": config_4d.templates[0],\n",
    "}\n",
    "\n",
    "output_4d = model_4d.new_trace(input_sample_4d)\n",
    "prompt_4d = output_4d['raw_input']\n",
    "input_sample_4d['raw_input'] = prompt_4d  # Add raw_input to sample for later use\n",
    "\n",
    "print(f\"4-Digit Example: '{prompt_4d}'\\n\")\n",
    "\n",
    "# Tokenize with both models using pipeline.load()\n",
    "llama_token_dict_4d = llama_pipeline.load(\n",
    "    {\"raw_input\": prompt_4d}, add_special_tokens=False\n",
    ")\n",
    "llama_tokens_4d = llama_token_dict_4d[\"input_ids\"][0].tolist()\n",
    "\n",
    "gemma_token_dict_4d = gemma_pipeline.load(\n",
    "    {\"raw_input\": prompt_4d}, add_special_tokens=False\n",
    ")\n",
    "gemma_tokens_4d = gemma_token_dict_4d[\"input_ids\"][0].tolist()\n",
    "\n",
    "print(\"Llama 3.1 8B (4-digit):\")\n",
    "for i, tok_id in enumerate(llama_tokens_4d):\n",
    "    tok = llama_pipeline.tokenizer.decode([tok_id])\n",
    "    marker = (\n",
    "        \" <-- NUMBER TOKEN\"\n",
    "        if any(d in tok for d in [\"12\", \"34\", \"56\", \"78\", \"1234\", \"5678\"])\n",
    "        else \"\"\n",
    "    )\n",
    "    print(f\"  [{i}] {repr(tok)}{marker}\")\n",
    "\n",
    "print(\"\\nGemma 2 9B (4-digit):\")\n",
    "for i, tok_id in enumerate(gemma_tokens_4d):\n",
    "    tok = gemma_pipeline.tokenizer.decode([tok_id])\n",
    "    marker = (\n",
    "        \" <-- DIGIT\" if tok.strip() in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] else \"\"\n",
    "    )\n",
    "    print(f\"  [{i}] {repr(tok)}{marker}\")\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  Llama: NOW splits into 2 tokens per number (e.g., '12' and '34' for '1234')\")\n",
    "print(\"  Gemma: Still uses 4 tokens per number (digit-by-digit as always)\")\n",
    "print(\"\\nLlama's vocabulary has limits - very large numbers get split!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: The Configuration Lookup Table\n",
    "\n",
    "The `tokenization_config.py` module provides pre-computed information about how many tokens each model uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZATION CONFIGURATION LOOKUP\n",
      "======================================================================\n",
      "\n",
      "Llama configuration:\n",
      "{2: 1, 3: 1, 4: 2}\n",
      "  Interpretation: 2-digit → 1 token, 3-digit → 1 token, 4-digit → 2 tokens\n",
      "\n",
      "Gemma configuration:\n",
      "{2: 2, 3: 3, 4: 4}\n",
      "  Interpretation: 2-digit → 2 tokens, 3-digit → 3 tokens, 4-digit → 4 tokens\n",
      "\n",
      "======================================================================\n",
      "Using the helper function:\n",
      "======================================================================\n",
      "2-digit numbers:\n",
      "  Llama: 1 token(s) per number\n",
      "  Gemma: 2 token(s) per number\n",
      "\n",
      "3-digit numbers:\n",
      "  Llama: 1 token(s) per number\n",
      "  Gemma: 3 token(s) per number\n",
      "\n",
      "4-digit numbers:\n",
      "  Llama: 2 token(s) per number\n",
      "  Gemma: 4 token(s) per number\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TOKENIZATION CONFIGURATION LOOKUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nLlama configuration:\")\n",
    "print(TOKENIZATION_CONFIG[\"llama\"])\n",
    "print(\"  Interpretation: 2-digit → 1 token, 3-digit → 1 token, 4-digit → 2 tokens\")\n",
    "\n",
    "print(\"\\nGemma configuration:\")\n",
    "print(TOKENIZATION_CONFIG[\"gemma\"])\n",
    "print(\"  Interpretation: 2-digit → 2 tokens, 3-digit → 3 tokens, 4-digit → 4 tokens\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Using the helper function:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for num_digits in [2, 3, 4]:\n",
    "    llama_count = get_tokens_per_number(\n",
    "        \"meta-llama/Meta-Llama-3.1-8B-Instruct\", num_digits\n",
    "    )\n",
    "    gemma_count = get_tokens_per_number(\"google/gemma-2-9b\", num_digits)\n",
    "    print(f\"{num_digits}-digit numbers:\")\n",
    "    print(f\"  Llama: {llama_count} token(s) per number\")\n",
    "    print(f\"  Gemma: {gemma_count} token(s) per number\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Creating Token Positions for Experiments\n",
    "\n",
    "The `create_token_positions` function sets up all the token position objects we need for interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Positions for 2-Number, 2-Digit Addition\n",
      "======================================================================\n",
      "\n",
      "Available token positions:\n",
      "['digit_0_0', 'digit_0_1', 'digit_1_0', 'digit_1_1', 'delimiter_and', 'delimiter_is', 'last_token']\n",
      "\n",
      "These include:\n",
      "  - digit_0_0, digit_0_1: First number digits\n",
      "  - digit_1_0, digit_1_1: Second number digits\n",
      "  - delimiter_and: The ' and ' between numbers\n",
      "  - delimiter_is: The ' is' before the answer\n",
      "  - last_token: The last token (where model generates from)\n",
      "\n",
      "======================================================================\n",
      "These TokenPosition objects are used in intervention experiments!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create token positions for 2-number, 2-digit addition\n",
    "llama_positions = create_token_positions(llama_pipeline, num_addends=2, num_digits=2)\n",
    "gemma_positions = create_token_positions(gemma_pipeline, num_addends=2, num_digits=2)\n",
    "\n",
    "print(\"Token Positions for 2-Number, 2-Digit Addition\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nAvailable token positions:\")\n",
    "print(list(llama_positions.keys()))\n",
    "\n",
    "print(\"\\nThese include:\")\n",
    "print(\"  - digit_0_0, digit_0_1: First number digits\")\n",
    "print(\"  - digit_1_0, digit_1_1: Second number digits\")\n",
    "print(\"  - delimiter_and: The ' and ' between numbers\")\n",
    "print(\"  - delimiter_is: The ' is' before the answer\")\n",
    "print(\"  - last_token: The last token (where model generates from)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"These TokenPosition objects are used in intervention experiments!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Testing with Multiple Random Examples\n",
    "\n",
    "Let's verify the token position functions work correctly across different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing token position functions across random examples\n",
      "======================================================================\n",
      "\n",
      "Example 1: The sum of 42 and 21 is\n",
      "  Looking for digit_0_0 (tens place of first number):\n",
      "    Llama found: '42' at position [251]\n",
      "    Gemma found: '4' at position [249, 250]\n",
      "\n",
      "Example 2: The sum of 91 and 90 is\n",
      "  Looking for digit_0_0 (tens place of first number):\n",
      "    Llama found: '91' at position [251]\n",
      "    Gemma found: '9' at position [249, 250]\n",
      "\n",
      "Example 3: The sum of 68 and 31 is\n",
      "  Looking for digit_0_0 (tens place of first number):\n",
      "    Llama found: '68' at position [251]\n",
      "    Gemma found: '6' at position [249, 250]\n",
      "\n",
      "✓ Token position functions work correctly across all examples!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing token position functions across random examples\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(3):\n",
    "    # Generate random 2-digit addition problem\n",
    "    sample = sample_valid_addition_input(config, num_addends=2, num_digits=2)\n",
    "    output = model.new_trace(sample)\n",
    "    sample['raw_input'] = output['raw_input']  # Add raw_input to sample\n",
    "    \n",
    "    print(f\"\\nExample {i+1}: {output['raw_input']}\")\n",
    "    \n",
    "    # Get digit positions\n",
    "    llama_d00 = get_digit_token_position(sample, llama_pipeline, 0, 0)\n",
    "    gemma_d00 = get_digit_token_position(sample, gemma_pipeline, 0, 0)\n",
    "\n",
    "    # Tokenize using pipeline.load() for consistency\n",
    "    llama_token_dict_test = llama_pipeline.load(\n",
    "        {\"raw_input\": output[\"raw_input\"]}, add_special_tokens=False\n",
    "    )\n",
    "    llama_toks = llama_token_dict_test[\"input_ids\"][0].tolist()\n",
    "\n",
    "    gemma_token_dict_test = gemma_pipeline.load(\n",
    "        {\"raw_input\": output[\"raw_input\"]}, add_special_tokens=False\n",
    "    )\n",
    "    gemma_toks = gemma_token_dict_test[\"input_ids\"][0].tolist()\n",
    "\n",
    "    # Show what we found\n",
    "    llama_found = llama_pipeline.tokenizer.decode([llama_toks[llama_d00[0]]])\n",
    "    gemma_found = gemma_pipeline.tokenizer.decode([gemma_toks[gemma_d00[0]]])\n",
    "\n",
    "    print(\"  Looking for digit_0_0 (tens place of first number):\")\n",
    "    print(f\"    Llama found: {repr(llama_found)} at position {llama_d00}\")\n",
    "    print(f\"    Gemma found: {repr(gemma_found)} at position {gemma_d00}\")\n",
    "\n",
    "print(\"\\n✓ Token position functions work correctly across all examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Visual Token Highlighting with TokenPosition Objects\n",
    "\n",
    "Now let's use the `highlight_selected_token` method to visualize which tokens are selected for each digit position across different number sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GEMMA 2 9B: Highlighted Token Selections\n",
      "================================================================================\n",
      "\n",
      "### 2-Digit Addition: 23 + 45\n",
      "Full prompt: 'The sum of 23 and 45 is'\n",
      "\n",
      "digit_0_0: <bos>The sum of **2****3** and 45 is\n",
      "digit_0_1: <bos>The sum of **2****3** and 45 is\n",
      "digit_1_0: <bos>The sum of 23 and **4****5** is\n",
      "digit_1_1: <bos>The sum of 23 and **4****5** is\n",
      "\n",
      "delimiter_and: <bos>The sum of 23** and**** **45 is\n",
      "delimiter_is: <bos>The sum of 23 and 45** is**\n",
      "last_token: <bos>The sum of 23 and 45** is**\n",
      "\n",
      "\n",
      "### 3-Digit Addition: 123 + 456\n",
      "Full prompt: 'The sum of 123 and 456 is'\n",
      "\n",
      "digit_0_0: <bos>The sum of **1****2****3** and 456 is\n",
      "digit_0_1: <bos>The sum of **1****2****3** and 456 is\n",
      "digit_0_2: <bos>The sum of **1****2****3** and 456 is\n",
      "digit_1_0: <bos>The sum of 123 and **4****5****6** is\n",
      "digit_1_1: <bos>The sum of 123 and **4****5****6** is\n",
      "digit_1_2: <bos>The sum of 123 and **4****5****6** is\n",
      "\n",
      "\n",
      "### 4-Digit Addition: 1234 + 5678\n",
      "Full prompt: 'The sum of 1234 and 5678 is'\n",
      "\n",
      "digit_0_0: <bos>The sum of **1****2****3****4** and 5678 is\n",
      "digit_0_1: <bos>The sum of **1****2****3****4** and 5678 is\n",
      "digit_0_2: <bos>The sum of **1****2****3****4** and 5678 is\n",
      "digit_0_3: <bos>The sum of **1****2****3****4** and 5678 is\n",
      "digit_1_0: <bos>The sum of 1234 and **5****6****7****8** is\n",
      "digit_1_1: <bos>The sum of 1234 and **5****6****7****8** is\n",
      "digit_1_2: <bos>The sum of 1234 and **5****6****7****8** is\n",
      "digit_1_3: <bos>The sum of 1234 and **5****6****7****8** is\n",
      "\n",
      "================================================================================\n",
      "Notice how Gemma selects individual digit tokens!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GEMMA 2 9B: Highlighted Token Selections\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2-digit example\n",
    "print(\"\\n### 2-Digit Addition: 23 + 45\")\n",
    "print(f\"Full prompt: '{prompt}'\")\n",
    "print()\n",
    "\n",
    "gemma_positions_2d = create_token_positions(gemma_pipeline, num_addends=2, num_digits=2)\n",
    "\n",
    "for digit_name in [\"digit_0_0\", \"digit_0_1\", \"digit_1_0\", \"digit_1_1\"]:\n",
    "    token_pos = gemma_positions_2d[digit_name]\n",
    "    highlighted = token_pos.highlight_selected_token(input_sample)\n",
    "    print(f\"{digit_name}: {highlighted}\")\n",
    "\n",
    "print(\n",
    "    f\"\\ndelimiter_and: {gemma_positions_2d['delimiter_and'].highlight_selected_token(input_sample)}\"\n",
    ")\n",
    "print(\n",
    "    f\"delimiter_is: {gemma_positions_2d['delimiter_is'].highlight_selected_token(input_sample)}\"\n",
    ")\n",
    "print(\n",
    "    f\"last_token: {gemma_positions_2d['last_token'].highlight_selected_token(input_sample)}\"\n",
    ")\n",
    "\n",
    "# 3-digit example\n",
    "print(\"\\n\\n### 3-Digit Addition: 123 + 456\")\n",
    "print(f\"Full prompt: '{prompt_3d}'\")\n",
    "print()\n",
    "\n",
    "gemma_positions_3d = create_token_positions(gemma_pipeline, num_addends=2, num_digits=3)\n",
    "\n",
    "for digit_name in [\n",
    "    \"digit_0_0\",\n",
    "    \"digit_0_1\",\n",
    "    \"digit_0_2\",\n",
    "    \"digit_1_0\",\n",
    "    \"digit_1_1\",\n",
    "    \"digit_1_2\",\n",
    "]:\n",
    "    token_pos = gemma_positions_3d[digit_name]\n",
    "    highlighted = token_pos.highlight_selected_token(input_sample_3d)\n",
    "    print(f\"{digit_name}: {highlighted}\")\n",
    "\n",
    "# 4-digit example\n",
    "print(\"\\n\\n### 4-Digit Addition: 1234 + 5678\")\n",
    "print(f\"Full prompt: '{prompt_4d}'\")\n",
    "print()\n",
    "\n",
    "gemma_positions_4d = create_token_positions(gemma_pipeline, num_addends=2, num_digits=4)\n",
    "\n",
    "for digit_name in [\n",
    "    \"digit_0_0\",\n",
    "    \"digit_0_1\",\n",
    "    \"digit_0_2\",\n",
    "    \"digit_0_3\",\n",
    "    \"digit_1_0\",\n",
    "    \"digit_1_1\",\n",
    "    \"digit_1_2\",\n",
    "    \"digit_1_3\",\n",
    "]:\n",
    "    token_pos = gemma_positions_4d[digit_name]\n",
    "    highlighted = token_pos.highlight_selected_token(input_sample_4d)\n",
    "    print(f\"{digit_name}: {highlighted}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Notice how Gemma selects individual digit tokens!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLAMA 3.1 8B: Highlighted Token Selections\n",
      "================================================================================\n",
      "\n",
      "### 2-Digit Addition: 23 + 45\n",
      "Full prompt: 'The sum of 23 and 45 is'\n",
      "\n",
      "digit_0_0: <|begin_of_text|>The sum of **23** and 45 is\n",
      "digit_0_1: <|begin_of_text|>The sum of **23** and 45 is\n",
      "digit_1_0: <|begin_of_text|>The sum of 23 and **45** is\n",
      "digit_1_1: <|begin_of_text|>The sum of 23 and **45** is\n",
      "\n",
      "delimiter_and: <|begin_of_text|>The sum of 23** and**** **45 is\n",
      "delimiter_is: <|begin_of_text|>The sum of 23 and 45** is**\n",
      "last_token: <|begin_of_text|>The sum of 23 and 45** is**\n",
      "\n",
      "\n",
      "### 3-Digit Addition: 123 + 456\n",
      "Full prompt: 'The sum of 123 and 456 is'\n",
      "\n",
      "digit_0_0: <|begin_of_text|>The sum of **123** and 456 is\n",
      "digit_0_1: <|begin_of_text|>The sum of **123** and 456 is\n",
      "digit_0_2: <|begin_of_text|>The sum of **123** and 456 is\n",
      "digit_1_0: <|begin_of_text|>The sum of 123 and **456** is\n",
      "digit_1_1: <|begin_of_text|>The sum of 123 and **456** is\n",
      "digit_1_2: <|begin_of_text|>The sum of 123 and **456** is\n",
      "\n",
      "\n",
      "### 4-Digit Addition: 1234 + 5678\n",
      "Full prompt: 'The sum of 1234 and 5678 is'\n",
      "\n",
      "digit_0_0: <|begin_of_text|>The sum of **123****4** and 5678 is\n",
      "digit_0_1: <|begin_of_text|>The sum of **123****4** and 5678 is\n",
      "digit_0_2: <|begin_of_text|>The sum of **123****4** and 5678 is\n",
      "digit_0_3: <|begin_of_text|>The sum of **123****4** and 5678 is\n",
      "digit_1_0: <|begin_of_text|>The sum of 1234 and **567****8** is\n",
      "digit_1_1: <|begin_of_text|>The sum of 1234 and **567****8** is\n",
      "digit_1_2: <|begin_of_text|>The sum of 1234 and **567****8** is\n",
      "digit_1_3: <|begin_of_text|>The sum of 1234 and **567****8** is\n",
      "\n",
      "================================================================================\n",
      "Notice how Llama selects the SAME whole-number token for multiple digits!\n",
      "For 2-digit and 3-digit numbers: all digit positions select the entire number token\n",
      "For 4-digit numbers: digits are split into pairs (e.g., '12' and '34')\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LLAMA 3.1 8B: Highlighted Token Selections\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2-digit example\n",
    "print(\"\\n### 2-Digit Addition: 23 + 45\")\n",
    "print(f\"Full prompt: '{prompt}'\")\n",
    "print()\n",
    "\n",
    "llama_positions_2d = create_token_positions(llama_pipeline, num_addends=2, num_digits=2)\n",
    "\n",
    "for digit_name in [\"digit_0_0\", \"digit_0_1\", \"digit_1_0\", \"digit_1_1\"]:\n",
    "    token_pos = llama_positions_2d[digit_name]\n",
    "    highlighted = token_pos.highlight_selected_token(input_sample)\n",
    "    print(f\"{digit_name}: {highlighted}\")\n",
    "\n",
    "print(\n",
    "    f\"\\ndelimiter_and: {llama_positions_2d['delimiter_and'].highlight_selected_token(input_sample)}\"\n",
    ")\n",
    "print(\n",
    "    f\"delimiter_is: {llama_positions_2d['delimiter_is'].highlight_selected_token(input_sample)}\"\n",
    ")\n",
    "print(\n",
    "    f\"last_token: {llama_positions_2d['last_token'].highlight_selected_token(input_sample)}\"\n",
    ")\n",
    "\n",
    "# 3-digit example\n",
    "print(\"\\n\\n### 3-Digit Addition: 123 + 456\")\n",
    "print(f\"Full prompt: '{prompt_3d}'\")\n",
    "print()\n",
    "\n",
    "llama_positions_3d = create_token_positions(llama_pipeline, num_addends=2, num_digits=3)\n",
    "\n",
    "for digit_name in [\n",
    "    \"digit_0_0\",\n",
    "    \"digit_0_1\",\n",
    "    \"digit_0_2\",\n",
    "    \"digit_1_0\",\n",
    "    \"digit_1_1\",\n",
    "    \"digit_1_2\",\n",
    "]:\n",
    "    token_pos = llama_positions_3d[digit_name]\n",
    "    highlighted = token_pos.highlight_selected_token(input_sample_3d)\n",
    "    print(f\"{digit_name}: {highlighted}\")\n",
    "\n",
    "# 4-digit example\n",
    "print(\"\\n\\n### 4-Digit Addition: 1234 + 5678\")\n",
    "print(f\"Full prompt: '{prompt_4d}'\")\n",
    "print()\n",
    "\n",
    "llama_positions_4d = create_token_positions(llama_pipeline, num_addends=2, num_digits=4)\n",
    "\n",
    "for digit_name in [\n",
    "    \"digit_0_0\",\n",
    "    \"digit_0_1\",\n",
    "    \"digit_0_2\",\n",
    "    \"digit_0_3\",\n",
    "    \"digit_1_0\",\n",
    "    \"digit_1_1\",\n",
    "    \"digit_1_2\",\n",
    "    \"digit_1_3\",\n",
    "]:\n",
    "    token_pos = llama_positions_4d[digit_name]\n",
    "    highlighted = token_pos.highlight_selected_token(input_sample_4d)\n",
    "    print(f\"{digit_name}: {highlighted}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Notice how Llama selects the SAME whole-number token for multiple digits!\")\n",
    "print(\n",
    "    \"For 2-digit and 3-digit numbers: all digit positions select the entire number token\"\n",
    ")\n",
    "print(\"For 4-digit numbers: digits are split into pairs (e.g., '12' and '34')\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### Tokenization Patterns\n",
    "\n",
    "| Model | 2-digit | 3-digit | 4-digit | Pattern |\n",
    "|-------|---------|---------|---------|----------|\n",
    "| **Llama 3.1 8B** | 1 token | 1 token | 2 tokens | Whole numbers (with limits) |\n",
    "| **Gemma 2 9B** | 2 tokens | 3 tokens | 4 tokens | Digit-by-digit |\n",
    "\n",
    "### Implications for Interventions\n",
    "\n",
    "1. **Llama 3.1 8B**:\n",
    "   - Uses compact whole-number tokens\n",
    "   - Interventions affect entire numbers, not individual digits\n",
    "   - Tests how model handles number-level representations\n",
    "   - Cannot test fine-grained digit operations\n",
    "\n",
    "2. **Gemma 2 9B**:\n",
    "   - Uses digit-by-digit tokenization\n",
    "   - Interventions can target individual digits precisely\n",
    "   - Tests how model handles digit-level operations\n",
    "   - Better for testing carry propagation hypotheses\n",
    "\n",
    "### Critical Insight\n",
    "\n",
    "**Both tokenization schemes are valid, but they test different aspects of the models!**\n",
    "\n",
    "- When we intervene on \"digit_0_0\" with Llama, we're really testing whole-number processing\n",
    "- When we intervene on \"digit_0_0\" with Gemma, we're truly testing individual digit processing\n",
    "- The `tokenization_config.py` ensures experiments account for these differences\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll use these token positions to:\n",
    "1. Generate counterfactual datasets\n",
    "2. Test different computational hypotheses (basic vs intermediate models)\n",
    "3. Run actual neural interventions to see which variables the models represent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}